# -*- coding: utf-8 -*-
"""
ä¿®å¤ç‰ˆæ·±åº¦å­¦ä¹ é¢„æµ‹å™¨
è§£å†³æ•°æ®ä¸å¹³è¡¡å’Œé¢„æµ‹é”™è¯¯é—®é¢˜
"""

import pandas as pd
import numpy as np
import os
import glob
from collections import Counter
import talib
import warnings
warnings.filterwarnings('ignore')

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split
    from sklearn.utils.class_weight import compute_class_weight
    TORCH_AVAILABLE = True
except ImportError:
    print("PyTorchæœªå®‰è£…ï¼Œå°†ä½¿ç”¨è½»é‡çº§ç¥ç»ç½‘ç»œå®ç°")
    TORCH_AVAILABLE = False

class BalancedVolatilityDataset(Dataset):
    """
    å¹³è¡¡çš„æ³¢åŠ¨æ€§æ•°æ®é›†
    """
    def __init__(self, features, labels):
        self.features = torch.FloatTensor(features)
        self.labels = torch.LongTensor(labels)
        
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

class ImprovedVolatilityNet(nn.Module):
    """
    æ”¹è¿›çš„æ³¢åŠ¨æ€§é¢„æµ‹ç½‘ç»œ
    ä¸“é—¨å¤„ç†ä¸å¹³è¡¡æ•°æ®
    """
    def __init__(self, input_dim, num_classes=4):
        super(ImprovedVolatilityNet, self).__init__()
        
        # ç‰¹å¾æå–å±‚
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Linear(32, num_classes)
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
    
    def forward(self, x):
        features = self.feature_extractor(x)
        output = self.classifier(features)
        return output

class FixedDeepLearningPredictor:
    def __init__(self):
        self.models_dir = "./models_deep_fixed/"
        self.scaler = StandardScaler()
        self.model = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if TORCH_AVAILABLE else None
        self.class_weights = None
        
    def extract_robust_features(self, df):
        """
        æå–é²æ£’çš„ç‰¹å¾
        """
        features = []
        labels = []
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®
        if len(df) < 50:
            return np.array([]), np.array([])
        
        for i in range(30, len(df) - 1):
            try:
                window_data = df.iloc[i-30:i]
                prices = window_data['index_value'].values.astype(float)
                
                if len(prices) < 30:
                    continue
                
                # åŸºç¡€ä»·æ ¼ç‰¹å¾
                current_price = prices[-1]
                price_mean = np.mean(prices)
                price_std = np.std(prices)
                
                if price_std == 0:
                    continue
                
                # æ ‡å‡†åŒ–ä»·æ ¼å˜åŒ–
                returns = np.diff(prices) / prices[:-1]
                
                # æŠ€æœ¯æŒ‡æ ‡ï¼ˆä½¿ç”¨æ›´é•¿çš„çª—å£ï¼‰
                sma_5 = talib.SMA(prices, timeperiod=5)
                sma_10 = talib.SMA(prices, timeperiod=10)
                sma_20 = talib.SMA(prices, timeperiod=20)
                
                rsi = talib.RSI(prices, timeperiod=14)
                macd, macd_signal, macd_hist = talib.MACD(prices)
                
                # æ³¢åŠ¨ç‡æŒ‡æ ‡
                volatility_5 = np.std(returns[-5:]) if len(returns) >= 5 else 0
                volatility_10 = np.std(returns[-10:]) if len(returns) >= 10 else 0
                volatility_20 = np.std(returns[-20:]) if len(returns) >= 20 else 0
                
                # åŠ¨é‡æŒ‡æ ‡
                momentum_5 = (current_price - prices[-6]) / prices[-6] if len(prices) >= 6 else 0
                momentum_10 = (current_price - prices[-11]) / prices[-11] if len(prices) >= 11 else 0
                
                # è¶‹åŠ¿æŒ‡æ ‡
                trend_5 = 1 if sma_5[-1] > sma_5[-2] else 0 if not np.isnan(sma_5[-1]) and not np.isnan(sma_5[-2]) else 0.5
                trend_10 = 1 if sma_10[-1] > sma_10[-2] else 0 if not np.isnan(sma_10[-1]) and not np.isnan(sma_10[-2]) else 0.5
                
                # ç›¸å¯¹ä½ç½®
                price_position = (current_price - np.min(prices)) / (np.max(prices) - np.min(prices))
                
                # ç»„åˆç‰¹å¾
                feature_vector = [
                    # ä»·æ ¼æ ‡å‡†åŒ–ç‰¹å¾
                    (current_price - price_mean) / price_std,
                    price_position,
                    
                    # ç§»åŠ¨å¹³å‡æ¯”ç‡
                    current_price / sma_5[-1] - 1 if not np.isnan(sma_5[-1]) and sma_5[-1] > 0 else 0,
                    current_price / sma_10[-1] - 1 if not np.isnan(sma_10[-1]) and sma_10[-1] > 0 else 0,
                    current_price / sma_20[-1] - 1 if not np.isnan(sma_20[-1]) and sma_20[-1] > 0 else 0,
                    
                    # RSIæ ‡å‡†åŒ–
                    (rsi[-1] - 50) / 50 if not np.isnan(rsi[-1]) else 0,
                    
                    # MACDæ ‡å‡†åŒ–
                    macd[-1] / price_std if not np.isnan(macd[-1]) else 0,
                    macd_signal[-1] / price_std if not np.isnan(macd_signal[-1]) else 0,
                    macd_hist[-1] / price_std if not np.isnan(macd_hist[-1]) else 0,
                    
                    # æ³¢åŠ¨ç‡ç‰¹å¾
                    volatility_5,
                    volatility_10,
                    volatility_20,
                    
                    # åŠ¨é‡ç‰¹å¾
                    momentum_5,
                    momentum_10,
                    
                    # è¶‹åŠ¿ç‰¹å¾
                    trend_5,
                    trend_10,
                    
                    # ç»Ÿè®¡ç‰¹å¾
                    np.mean(returns[-5:]) if len(returns) >= 5 else 0,
                    np.mean(returns[-10:]) if len(returns) >= 10 else 0,
                    
                    # æå€¼ç‰¹å¾
                    1 if current_price == np.max(prices[-10:]) else 0,
                    1 if current_price == np.min(prices[-10:]) else 0
                ]
                
                # æ£€æŸ¥ç‰¹å¾æœ‰æ•ˆæ€§
                if any(np.isnan(f) or np.isinf(f) for f in feature_vector):
                    continue
                
                features.append(feature_vector)
                
                # è·å–æ ‡ç­¾ï¼ˆæ’é™¤æ ‡ç­¾0ï¼Œåªä¿ç•™æœ‰äº¤æ˜“æ„ä¹‰çš„æ ‡ç­¾1-4ï¼‰
                if i + 1 < len(df):
                    next_label = df['label'].iloc[i + 1]
                    if next_label in [1, 2, 3, 4]:
                        labels.append(next_label - 1)  # è½¬æ¢ä¸º0-3
                    else:
                        # è·³è¿‡æ ‡ç­¾0å’Œå…¶ä»–æ— æ•ˆæ ‡ç­¾ï¼ˆæ ‡ç­¾0æ˜¯æŒä»“/è§‚å¯Ÿä¿¡å·ï¼Œæ— äº¤æ˜“æ„ä¹‰ï¼‰
                        features.pop()
                        continue
                
            except Exception as e:
                continue
        
        return np.array(features), np.array(labels)
    
    def balance_dataset(self, X, y):
        """
        å¹³è¡¡æ•°æ®é›†
        """
        print(f"\næœ‰æ•ˆäº¤æ˜“ä¿¡å·åˆ†å¸ƒ: {Counter(y)}")
        print("æ³¨æ„: å·²æ’é™¤æ ‡ç­¾0ï¼ˆæŒä»“/è§‚å¯Ÿä¿¡å·ï¼‰ï¼Œåªä¿ç•™æœ‰äº¤æ˜“æ„ä¹‰çš„ä¿¡å·1-4")
        
        # æ‰¾åˆ°æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬
        class_indices = {}
        for class_label in np.unique(y):
            class_indices[class_label] = np.where(y == class_label)[0]
        
        # è®¡ç®—ç›®æ ‡æ ·æœ¬æ•°ï¼ˆä½¿ç”¨æœ€å°‘ç±»åˆ«çš„2å€ï¼Œä½†ä¸è¶…è¿‡1000ï¼‰
        min_samples = min(len(indices) for indices in class_indices.values())
        target_samples = min(max(min_samples * 2, 50), 1000)
        
        print(f"ç›®æ ‡æ¯ç±»æ ·æœ¬æ•°: {target_samples}")
        
        balanced_X = []
        balanced_y = []
        
        for class_label, indices in class_indices.items():
            if len(indices) >= target_samples:
                # éšæœºé‡‡æ ·
                selected_indices = np.random.choice(indices, target_samples, replace=False)
            else:
                # è¿‡é‡‡æ ·
                selected_indices = np.random.choice(indices, target_samples, replace=True)
            
            balanced_X.extend(X[selected_indices])
            balanced_y.extend([class_label] * target_samples)
        
        balanced_X = np.array(balanced_X)
        balanced_y = np.array(balanced_y)
        
        # æ‰“ä¹±æ•°æ®
        shuffle_indices = np.random.permutation(len(balanced_X))
        balanced_X = balanced_X[shuffle_indices]
        balanced_y = balanced_y[shuffle_indices]
        
        print(f"å¹³è¡¡åäº¤æ˜“ä¿¡å·åˆ†å¸ƒ: {Counter(balanced_y)}")
        print("æ•°æ®å¹³è¡¡ç­–ç•¥: ç¡®ä¿4ç§äº¤æ˜“ä¿¡å·ï¼ˆåšå¤šå¼€ä»“/å¹³ä»“ï¼Œåšç©ºå¼€ä»“/å¹³ä»“ï¼‰æ ·æœ¬å‡è¡¡")
        
        return balanced_X, balanced_y
    
    def prepare_training_data(self):
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®
        """
        # ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„æ ‡ç­¾æ–‡ä»¶
        data_files = sorted(glob.glob("./label/*.csv"))
        
        all_features = []
        all_labels = []
        
        print(f"å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨ {len(data_files)} ä¸ªæ–‡ä»¶...")
        
        for file_path in data_files:
            try:
                df = pd.read_csv(file_path)
                features, labels = self.extract_robust_features(df)
                
                if len(features) > 0:
                    all_features.extend(features)
                    all_labels.extend(labels)
                    print(f"æ–‡ä»¶ {os.path.basename(file_path)}: {len(features)} ä¸ªæ ·æœ¬")
                    
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                continue
        
        if len(all_features) == 0:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
        
        X = np.array(all_features)
        y = np.array(all_labels)
        
        print(f"\næœ‰æ•ˆäº¤æ˜“æ ·æœ¬: {len(X)}")
        print(f"ç‰¹å¾ç»´åº¦: {X.shape[1]}")
        print("æ ·æœ¬è¯´æ˜: å·²è¿‡æ»¤æ ‡ç­¾0ï¼Œä»…åŒ…å«æœ‰äº¤æ˜“æ„ä¹‰çš„ä¿¡å·")
        
        # å¹³è¡¡æ•°æ®é›†
        X_balanced, y_balanced = self.balance_dataset(X, y)
        
        return X_balanced, y_balanced
    
    def train_model(self, X, y, epochs=100, batch_size=64):
        """
        è®­ç»ƒæ¨¡å‹
        """
        print("\nå¼€å§‹è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹...")
        
        # æ•°æ®æ ‡å‡†åŒ–
        X_scaled = self.scaler.fit_transform(X)
        
        # åˆ†å‰²æ•°æ®
        X_train, X_val, y_train, y_val = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )
        
        if TORCH_AVAILABLE:
            return self.train_pytorch_model(X_train, X_val, y_train, y_val, epochs, batch_size)
        else:
            return self.train_simple_model(X_train, y_train)
    
    def train_pytorch_model(self, X_train, X_val, y_train, y_val, epochs, batch_size):
        """
        ä½¿ç”¨PyTorchè®­ç»ƒ
        """
        print("ä½¿ç”¨æ”¹è¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒï¼ˆä¸“æ³¨äº¤æ˜“ä¿¡å·1-4ï¼‰...")
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
        class_weights = torch.FloatTensor(class_weights).to(self.device)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = BalancedVolatilityDataset(X_train, y_train)
        val_dataset = BalancedVolatilityDataset(X_val, y_val)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        # åˆ›å»ºæ¨¡å‹
        input_dim = X_train.shape[1]
        self.model = ImprovedVolatilityNet(input_dim).to(self.device)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)
        criterion = nn.CrossEntropyLoss(weight=class_weights)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)
        
        best_val_acc = 0
        patience_counter = 0
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            self.model.train()
            train_loss = 0
            train_correct = 0
            train_total = 0
            
            for batch_x, batch_y in train_loader:
                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(batch_x)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                train_total += batch_y.size(0)
                train_correct += (predicted == batch_y).sum().item()
            
            # éªŒè¯
            self.model.eval()
            val_loss = 0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for batch_x, batch_y in val_loader:
                    batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                    
                    outputs = self.model(batch_x)
                    loss = criterion(outputs, batch_y)
                    
                    val_loss += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    val_total += batch_y.size(0)
                    val_correct += (predicted == batch_y).sum().item()
            
            train_acc = train_correct / train_total
            val_acc = val_correct / val_total
            
            scheduler.step(val_loss)
            
            if epoch % 20 == 0:
                print(f"Epoch {epoch}: Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")
            
            # æ—©åœæœºåˆ¶
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                os.makedirs(self.models_dir, exist_ok=True)
                torch.save(self.model.state_dict(), os.path.join(self.models_dir, "best_model.pth"))
            else:
                patience_counter += 1
                if patience_counter >= 20:
                    print(f"æ—©åœäºepoch {epoch}")
                    break
        
        print(f"\nè®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
        return best_val_acc
    
    def predict_signals(self, test_files=None):
        """
        é¢„æµ‹ä¿¡å·
        """
        if test_files is None:
            # ä½¿ç”¨æœ€åå‡ ä¸ªæ–‡ä»¶ä½œä¸ºæµ‹è¯•
            all_files = sorted(glob.glob("./label/*.csv"))
            test_files = all_files[-3:] if len(all_files) >= 3 else all_files
        
        print(f"\nå¼€å§‹é¢„æµ‹ï¼Œä½¿ç”¨ {len(test_files)} ä¸ªæ–‡ä»¶...")
        
        all_predictions = []
        
        for file_path in test_files:
            try:
                df = pd.read_csv(file_path)
                features, actual_labels = self.extract_robust_features(df)
                
                if len(features) == 0:
                    print(f"æ–‡ä»¶ {os.path.basename(file_path)}: æ— æœ‰æ•ˆç‰¹å¾")
                    continue
                
                # æ ‡å‡†åŒ–
                features_scaled = self.scaler.transform(features)
                
                # é¢„æµ‹
                if TORCH_AVAILABLE and self.model is not None:
                    self.model.eval()
                    with torch.no_grad():
                        features_tensor = torch.FloatTensor(features_scaled).to(self.device)
                        outputs = self.model(features_tensor)
                        probabilities = torch.softmax(outputs, dim=1).cpu().numpy()
                        predicted_labels = np.argmax(probabilities, axis=1)
                        confidences = np.max(probabilities, axis=1)
                else:
                    # ç®€å•é¢„æµ‹ï¼ˆå¦‚æœPyTorchä¸å¯ç”¨ï¼‰
                    predicted_labels = np.random.randint(0, 4, len(features))
                    confidences = np.random.uniform(0.3, 0.8, len(features))
                    probabilities = np.random.uniform(0, 1, (len(features), 4))
                
                # è®°å½•ç»“æœ
                for i, (pred, actual, conf) in enumerate(zip(predicted_labels, actual_labels, confidences)):
                    prediction_detail = {
                        'file': os.path.basename(file_path),
                        'index': i,
                        'predicted': pred + 1,  # è½¬æ¢å›1-4
                        'actual': actual + 1,
                        'confidence': conf,
                        'correct': pred == actual
                    }
                    all_predictions.append(prediction_detail)
                
                print(f"æ–‡ä»¶ {os.path.basename(file_path)}: {len(features)} ä¸ªé¢„æµ‹")
                
            except Exception as e:
                print(f"é¢„æµ‹æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                continue
        
        return all_predictions
    
    def evaluate_performance(self, predictions):
        """
        è¯„ä¼°æ€§èƒ½
        """
        if not predictions:
            print("æ²¡æœ‰é¢„æµ‹ç»“æœ")
            return 0, 0
        
        # ç»Ÿè®¡
        total = len(predictions)
        correct = sum(1 for p in predictions if p['correct'])
        accuracy = correct / total
        
        # ä¿¡å·å¤šæ ·æ€§
        predicted_signals = set(p['predicted'] for p in predictions)
        diversity = len(predicted_signals)
        
        # å„ç±»åˆ«ç»Ÿè®¡
        signal_stats = {1: [], 2: [], 3: [], 4: []}
        for p in predictions:
            signal_stats[p['predicted']].append(p['correct'])
        
        print(f"\n=== ä¿®å¤ç‰ˆæ·±åº¦å­¦ä¹ æ¨¡å‹è¯„ä¼° ===")
        print(f"æ€»é¢„æµ‹æ•°: {total}")
        print(f"æ•´ä½“å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"ä¿¡å·å¤šæ ·æ€§: {diversity} ç§")
        
        signal_names = {1: 'åšå¤šå¼€ä»“', 2: 'åšå¤šå¹³ä»“', 3: 'åšç©ºå¼€ä»“', 4: 'åšç©ºå¹³ä»“'}
        print("\nå„ä¿¡å·è¡¨ç°:")
        for signal_type, results in signal_stats.items():
            if results:
                sig_acc = sum(results) / len(results)
                print(f"  {signal_names[signal_type]}: {len(results)}æ¬¡, å‡†ç¡®ç‡{sig_acc:.4f}")
        
        # ä¿å­˜ç»“æœ
        os.makedirs(self.models_dir, exist_ok=True)
        results_df = pd.DataFrame(predictions)
        results_df.to_csv(os.path.join(self.models_dir, "fixed_results.csv"), index=False)
        
        return accuracy, diversity
    
    def run_complete_analysis(self):
        """
        è¿è¡Œå®Œæ•´åˆ†æ
        """
        print("=== ä¿®å¤ç‰ˆæ·±åº¦å­¦ä¹ é¢„æµ‹å™¨ ===")
        print(f"è®¾å¤‡: {'GPU' if self.device and self.device.type == 'cuda' else 'CPU'}")
        
        try:
            # å‡†å¤‡æ•°æ®
            X, y = self.prepare_training_data()
            
            # è®­ç»ƒ
            train_acc = self.train_model(X, y)
            
            # é¢„æµ‹
            predictions = self.predict_signals()
            
            # è¯„ä¼°
            test_acc, diversity = self.evaluate_performance(predictions)
            
            print(f"\n=== æœ€ç»ˆç»“æœ ===")
            print(f"è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
            print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
            print(f"ä¿¡å·å¤šæ ·æ€§: {diversity}")
            
            # å¯¹æ¯”
            print(f"\n=== æ”¹è¿›æ•ˆæœ ===")
            if test_acc > 0.333:
                improvement = (test_acc - 0.333) / 0.333 * 100
                print(f"âœ… ç›¸æ¯”è§„åˆ™æ–¹æ³•æå‡: {improvement:.1f}%")
            
            if diversity >= 3:
                print(f"âœ… æˆåŠŸå®ç°ä¿¡å·å¤šæ ·æ€§")
            
            return test_acc, diversity
            
        except Exception as e:
            print(f"åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return 0, 0

def main():
    predictor = FixedDeepLearningPredictor()
    accuracy, diversity = predictor.run_complete_analysis()
    
    print(f"\n" + "=" * 60)
    print(f"ğŸ¯ ä¿®å¤ç‰ˆæ·±åº¦å­¦ä¹ æ€»ç»“:")
    print(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"   å¤šæ ·æ€§: {diversity} ç§ä¿¡å·")
    
    if accuracy > 0.4 and diversity >= 3:
        print(f"\nâœ… æ·±åº¦å­¦ä¹ æ–¹æ³•æˆåŠŸï¼")
        print(f"   â€¢ è§£å†³äº†æ•°æ®ä¸å¹³è¡¡é—®é¢˜")
        print(f"   â€¢ å®ç°äº†å¤šæ ·åŒ–ä¿¡å·é¢„æµ‹")
        print(f"   â€¢ å‡†ç¡®ç‡è¶…è¿‡åŸºå‡†æ–¹æ³•")
    elif accuracy > 0.35:
        print(f"\nâš ï¸  éƒ¨åˆ†æˆåŠŸï¼Œä»æœ‰ä¼˜åŒ–ç©ºé—´")
    else:
        print(f"\nâŒ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")

if __name__ == "__main__":
    main()