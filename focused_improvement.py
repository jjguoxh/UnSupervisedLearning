#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ— ç›‘ç£å­¦ä¹ äº¤æ˜“ä¿¡å·é¢„æµ‹ç³»ç»Ÿ - ä¸“æ³¨æ”¹è¿›å·¥å…·
åŸºäºäº¤æ˜“ä¿¡å·ç¨€ç–æ€§çš„åˆç†æ€§ï¼Œä¸“æ³¨æé«˜äº¤æ˜“ä¿¡å·é¢„æµ‹å‡†ç¡®æ€§
"""

import os
import json
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support
from collections import Counter
import pickle
from datetime import datetime

class FocusedImprovement:
    def __init__(self, base_dir="."):
        self.base_dir = base_dir
        self.data_dir = os.path.join(base_dir, "data")
        self.label_dir = os.path.join(base_dir, "label")
        self.model_dir = os.path.join(base_dir, "model")
        
    def analyze_trading_signal_quality(self):
        """åˆ†æäº¤æ˜“ä¿¡å·è´¨é‡ï¼Œä¸“æ³¨äº1-4ä¿¡å·"""
        print("\n=== äº¤æ˜“ä¿¡å·è´¨é‡åˆ†æ ===")
        
        all_labels = []
        trading_signals = []
        signal_contexts = []  # å­˜å‚¨ä¿¡å·å‰åçš„å¸‚åœºç¯å¢ƒ
        
        # æ”¶é›†æ‰€æœ‰æ ‡ç­¾æ•°æ®
        for file in os.listdir(self.label_dir):
            if file.endswith(".csv"):
                file_path = os.path.join(self.label_dir, file)
                try:
                    df = pd.read_csv(file_path)
                    labels = df['label'].values
                    all_labels.extend(labels)
                    
                    # åˆ†æäº¤æ˜“ä¿¡å·çš„ä¸Šä¸‹æ–‡
                    for i, label in enumerate(labels):
                        if label in [1, 2, 3, 4]:  # äº¤æ˜“ä¿¡å·
                            trading_signals.append(label)
                            
                            # æå–ä¿¡å·å‰åçš„å¸‚åœºç¯å¢ƒç‰¹å¾
                            context = {
                                'signal': label,
                                'position': i,
                                'total_length': len(labels)
                            }
                            
                            # æ·»åŠ ä»·æ ¼å˜åŒ–ä¿¡æ¯ï¼ˆå¦‚æœæœ‰index_valueåˆ—ï¼‰
                            if 'index_value' in df.columns and i > 0:
                                context['price_change'] = df.iloc[i]['index_value'] - df.iloc[i-1]['index_value']
                                
                                # è®¡ç®—ä¿¡å·åçš„ä»·æ ¼å˜åŒ–ï¼ˆç”¨äºéªŒè¯ä¿¡å·æœ‰æ•ˆæ€§ï¼‰
                                if i < len(df) - 10:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åç»­æ•°æ®
                                    future_prices = df.iloc[i:i+10]['index_value'].values
                                    context['future_return_5'] = (future_prices[4] - future_prices[0]) / future_prices[0] if len(future_prices) > 4 else 0
                                    context['future_return_10'] = (future_prices[-1] - future_prices[0]) / future_prices[0]
                            
                            signal_contexts.append(context)
                            
                except Exception as e:
                    print(f"è­¦å‘Š: æ— æ³•è¯»å– {file_path}: {e}")
        
        # åˆ†æç»“æœ
        total_points = len(all_labels)
        trading_signal_count = len(trading_signals)
        
        print(f"æ€»æ•°æ®ç‚¹: {total_points}")
        print(f"äº¤æ˜“ä¿¡å·æ•°é‡: {trading_signal_count}")
        print(f"äº¤æ˜“ä¿¡å·æ¯”ä¾‹: {trading_signal_count/total_points*100:.2f}%")
        
        signal_distribution = Counter(trading_signals)
        print("\näº¤æ˜“ä¿¡å·åˆ†å¸ƒ:")
        signal_names = {1: "åšå¤šå¼€ä»“", 2: "åšå¤šå¹³ä»“", 3: "åšç©ºå¼€ä»“", 4: "åšç©ºå¹³ä»“"}
        for signal, count in sorted(signal_distribution.items()):
            print(f"  {signal_names[signal]}({signal}): {count} æ¬¡")
        
        # åˆ†æä¿¡å·æœ‰æ•ˆæ€§ï¼ˆåŸºäºæœªæ¥æ”¶ç›Šï¼‰
        if signal_contexts:
            print("\nä¿¡å·æœ‰æ•ˆæ€§åˆ†æ:")
            for signal_type in [1, 2, 3, 4]:
                signal_data = [ctx for ctx in signal_contexts if ctx['signal'] == signal_type and 'future_return_5' in ctx]
                if signal_data:
                    returns_5 = [ctx['future_return_5'] for ctx in signal_data]
                    returns_10 = [ctx['future_return_10'] for ctx in signal_data]
                    
                    avg_return_5 = np.mean(returns_5) * 100
                    avg_return_10 = np.mean(returns_10) * 100
                    
                    print(f"  {signal_names[signal_type]}: 5æœŸå¹³å‡æ”¶ç›Š {avg_return_5:.2f}%, 10æœŸå¹³å‡æ”¶ç›Š {avg_return_10:.2f}%")
        
        return signal_contexts
    
    def create_trading_focused_features(self):
        """åˆ›å»ºä¸“æ³¨äºäº¤æ˜“ä¿¡å·çš„ç‰¹å¾"""
        print("\n=== åˆ›å»ºäº¤æ˜“ä¸“ç”¨ç‰¹å¾ ===")
        
        enhanced_features = []
        enhanced_labels = []
        
        for file in os.listdir(self.label_dir):
            if file.endswith(".csv"):
                file_path = os.path.join(self.label_dir, file)
                try:
                    df = pd.read_csv(file_path)
                    
                    for i in range(20, len(df)-20):  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å‰åæ•°æ®
                        label = df.iloc[i]['label']
                        
                        # åªå¤„ç†äº¤æ˜“ä¿¡å·å’Œéƒ¨åˆ†0ä¿¡å·ï¼ˆä½œä¸ºè´Ÿæ ·æœ¬ï¼‰
                        if label in [1, 2, 3, 4] or (label == 0 and np.random.random() < 0.1):
                            features = self.extract_enhanced_features(df, i)
                            if features is not None:
                                enhanced_features.append(features)
                                enhanced_labels.append(label)
                                
                except Exception as e:
                    print(f"è­¦å‘Š: å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
        
        print(f"âœ“ åˆ›å»ºäº† {len(enhanced_features)} ä¸ªå¢å¼ºç‰¹å¾æ ·æœ¬")
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {dict(Counter(enhanced_labels))}")
        
        return np.array(enhanced_features), np.array(enhanced_labels)
    
    def extract_enhanced_features(self, df, index):
        """æå–å¢å¼ºçš„äº¤æ˜“ç‰¹å¾"""
        try:
            features = []
            
            # åŸºç¡€ç‰¹å¾
            if 'a' in df.columns:
                features.extend([df.iloc[index]['a'], df.iloc[index]['b'], 
                               df.iloc[index]['c'], df.iloc[index]['d']])
            
            # ä»·æ ¼ç›¸å…³ç‰¹å¾
            if 'index_value' in df.columns:
                current_price = df.iloc[index]['index_value']
                
                # çŸ­æœŸä»·æ ¼å˜åŒ–
                price_changes = []
                for lookback in [1, 3, 5, 10]:
                    if index >= lookback:
                        past_price = df.iloc[index-lookback]['index_value']
                        change = (current_price - past_price) / past_price
                        price_changes.append(change)
                    else:
                        price_changes.append(0)
                
                features.extend(price_changes)
                
                # ä»·æ ¼æ³¢åŠ¨ç‡
                if index >= 10:
                    recent_prices = df.iloc[index-10:index]['index_value'].values
                    volatility = np.std(recent_prices) / np.mean(recent_prices)
                    features.append(volatility)
                else:
                    features.append(0)
                
                # è¶‹åŠ¿å¼ºåº¦
                if index >= 20:
                    long_prices = df.iloc[index-20:index]['index_value'].values
                    trend_strength = (long_prices[-1] - long_prices[0]) / long_prices[0]
                    features.append(trend_strength)
                else:
                    features.append(0)
            
            # æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
            if 'a' in df.columns and 'b' in df.columns:
                # ç®€å•ç§»åŠ¨å¹³å‡
                if index >= 5:
                    ma_a = np.mean(df.iloc[index-5:index]['a'].values)
                    ma_b = np.mean(df.iloc[index-5:index]['b'].values)
                    features.extend([ma_a, ma_b])
                    
                    # å½“å‰å€¼ä¸ç§»åŠ¨å¹³å‡çš„åç¦»
                    features.extend([
                        df.iloc[index]['a'] - ma_a,
                        df.iloc[index]['b'] - ma_b
                    ])
                else:
                    features.extend([0, 0, 0, 0])
            
            return features if len(features) > 0 else None
            
        except Exception as e:
            return None
    
    def train_specialized_models(self, X, y):
        """è®­ç»ƒä¸“é—¨çš„äº¤æ˜“ä¿¡å·é¢„æµ‹æ¨¡å‹"""
        print("\n=== è®­ç»ƒä¸“ç”¨äº¤æ˜“ä¿¡å·æ¨¡å‹ ===")
        
        # æ•°æ®é¢„å¤„ç†
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # åˆ†ç¦»äº¤æ˜“ä¿¡å·å’Œéäº¤æ˜“ä¿¡å·
        trading_mask = y != 0
        X_trading = X_scaled[trading_mask]
        y_trading = y[trading_mask]
        
        print(f"äº¤æ˜“ä¿¡å·æ ·æœ¬æ•°: {len(X_trading)}")
        print(f"äº¤æ˜“ä¿¡å·åˆ†å¸ƒ: {dict(Counter(y_trading))}")
        
        if len(X_trading) < 10:
            print("è­¦å‘Š: äº¤æ˜“ä¿¡å·æ ·æœ¬å¤ªå°‘ï¼Œæ— æ³•è®­ç»ƒæœ‰æ•ˆæ¨¡å‹")
            return None
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X_trading, y_trading, test_size=0.3, random_state=42, stratify=y_trading
        )
        
        # è®­ç»ƒå¤šä¸ªæ¨¡å‹
        models = {
            'RandomForest': RandomForestClassifier(
                n_estimators=200,
                max_depth=15,
                min_samples_split=3,
                min_samples_leaf=1,
                random_state=42,
                class_weight='balanced'
            ),
            'GradientBoosting': GradientBoostingClassifier(
                n_estimators=150,
                max_depth=8,
                learning_rate=0.1,
                random_state=42
            )
        }
        
        results = {}
        best_model = None
        best_score = 0
        
        for name, model in models.items():
            print(f"\nè®­ç»ƒ {name} æ¨¡å‹...")
            
            # äº¤å‰éªŒè¯
            cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring='f1_weighted')
            print(f"äº¤å‰éªŒè¯ F1 å¾—åˆ†: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
            
            # è®­ç»ƒæ¨¡å‹
            model.fit(X_train, y_train)
            
            # æµ‹è¯•é¢„æµ‹
            y_pred = model.predict(X_test)
            
            # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
            precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, average='weighted')
            
            print(f"æµ‹è¯•é›†æ€§èƒ½:")
            print(f"  ç²¾ç¡®ç‡: {precision:.4f}")
            print(f"  å¬å›ç‡: {recall:.4f}")
            print(f"  F1å¾—åˆ†: {f1:.4f}")
            
            # å„ç±»åˆ«è¯¦ç»†æŠ¥å‘Š
            print(f"\nå„ä¿¡å·ç±»åˆ«æ€§èƒ½:")
            report = classification_report(y_test, y_pred, output_dict=True)
            signal_names = {1: "åšå¤šå¼€ä»“", 2: "åšå¤šå¹³ä»“", 3: "åšç©ºå¼€ä»“", 4: "åšç©ºå¹³ä»“"}
            
            for signal in [1, 2, 3, 4]:
                if str(signal) in report:
                    metrics = report[str(signal)]
                    print(f"  {signal_names[signal]}: ç²¾ç¡®ç‡={metrics['precision']:.3f}, å¬å›ç‡={metrics['recall']:.3f}, F1={metrics['f1-score']:.3f}")
            
            # ä¿å­˜ç»“æœ
            results[name] = {
                'cv_f1_score': cv_scores.mean(),
                'test_precision': precision,
                'test_recall': recall,
                'test_f1': f1,
                'classification_report': report
            }
            
            # é€‰æ‹©æœ€ä½³æ¨¡å‹
            if f1 > best_score:
                best_score = f1
                best_model = (name, model, scaler)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if best_model:
            model_name, model, scaler = best_model
            
            specialized_dir = os.path.join(self.model_dir, "specialized")
            os.makedirs(specialized_dir, exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹
            model_file = os.path.join(specialized_dir, f"best_trading_model.pkl")
            with open(model_file, 'wb') as f:
                pickle.dump(model, f)
            
            # ä¿å­˜æ ‡å‡†åŒ–å™¨
            scaler_file = os.path.join(specialized_dir, f"trading_scaler.pkl")
            with open(scaler_file, 'wb') as f:
                pickle.dump(scaler, f)
            
            print(f"\nâœ“ æœ€ä½³æ¨¡å‹ ({model_name}) å·²ä¿å­˜ï¼ŒF1å¾—åˆ†: {best_score:.4f}")
        
        # ä¿å­˜è®­ç»ƒç»“æœ
        results_file = os.path.join(specialized_dir, "training_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        return results
    
    def create_trading_predictor(self):
        """åˆ›å»ºä¸“ç”¨çš„äº¤æ˜“ä¿¡å·é¢„æµ‹å™¨"""
        print("\n=== åˆ›å»ºäº¤æ˜“ä¿¡å·é¢„æµ‹å™¨ ===")
        
        predictor_code = '''
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸“ç”¨äº¤æ˜“ä¿¡å·é¢„æµ‹å™¨
ä¸“æ³¨äºé¢„æµ‹1-4äº¤æ˜“ä¿¡å·ï¼Œå¿½ç•¥0ä¿¡å·çš„å¹³è¡¡é—®é¢˜
"""

import os
import pickle
import numpy as np
import pandas as pd
from datetime import datetime

class TradingSignalPredictor:
    def __init__(self, model_dir="model/specialized"):
        self.model_dir = model_dir
        self.model = None
        self.scaler = None
        self.load_model()
    
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            model_file = os.path.join(self.model_dir, "best_trading_model.pkl")
            scaler_file = os.path.join(self.model_dir, "trading_scaler.pkl")
            
            with open(model_file, 'rb') as f:
                self.model = pickle.load(f)
            
            with open(scaler_file, 'rb') as f:
                self.scaler = pickle.load(f)
            
            print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def extract_features(self, df, index):
        """æå–é¢„æµ‹ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰"""
        try:
            features = []
            
            # åŸºç¡€ç‰¹å¾
            if 'a' in df.columns:
                features.extend([df.iloc[index]['a'], df.iloc[index]['b'], 
                               df.iloc[index]['c'], df.iloc[index]['d']])
            
            # ä»·æ ¼ç›¸å…³ç‰¹å¾
            if 'index_value' in df.columns:
                current_price = df.iloc[index]['index_value']
                
                # çŸ­æœŸä»·æ ¼å˜åŒ–
                price_changes = []
                for lookback in [1, 3, 5, 10]:
                    if index >= lookback:
                        past_price = df.iloc[index-lookback]['index_value']
                        change = (current_price - past_price) / past_price
                        price_changes.append(change)
                    else:
                        price_changes.append(0)
                
                features.extend(price_changes)
                
                # ä»·æ ¼æ³¢åŠ¨ç‡
                if index >= 10:
                    recent_prices = df.iloc[index-10:index]['index_value'].values
                    volatility = np.std(recent_prices) / np.mean(recent_prices)
                    features.append(volatility)
                else:
                    features.append(0)
                
                # è¶‹åŠ¿å¼ºåº¦
                if index >= 20:
                    long_prices = df.iloc[index-20:index]['index_value'].values
                    trend_strength = (long_prices[-1] - long_prices[0]) / long_prices[0]
                    features.append(trend_strength)
                else:
                    features.append(0)
            
            # æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
            if 'a' in df.columns and 'b' in df.columns:
                # ç®€å•ç§»åŠ¨å¹³å‡
                if index >= 5:
                    ma_a = np.mean(df.iloc[index-5:index]['a'].values)
                    ma_b = np.mean(df.iloc[index-5:index]['b'].values)
                    features.extend([ma_a, ma_b])
                    
                    # å½“å‰å€¼ä¸ç§»åŠ¨å¹³å‡çš„åç¦»
                    features.extend([
                        df.iloc[index]['a'] - ma_a,
                        df.iloc[index]['b'] - ma_b
                    ])
                else:
                    features.extend([0, 0, 0, 0])
            
            return np.array(features).reshape(1, -1) if len(features) > 0 else None
            
        except Exception as e:
            return None
    
    def predict_signal(self, df, index):
        """é¢„æµ‹äº¤æ˜“ä¿¡å·"""
        if self.model is None or self.scaler is None:
            return 0, 0.0
        
        features = self.extract_features(df, index)
        if features is None:
            return 0, 0.0
        
        try:
            # æ ‡å‡†åŒ–ç‰¹å¾
            features_scaled = self.scaler.transform(features)
            
            # é¢„æµ‹
            prediction = self.model.predict(features_scaled)[0]
            
            # è·å–é¢„æµ‹æ¦‚ç‡ï¼ˆç½®ä¿¡åº¦ï¼‰
            if hasattr(self.model, 'predict_proba'):
                probabilities = self.model.predict_proba(features_scaled)[0]
                confidence = np.max(probabilities)
            else:
                confidence = 0.5
            
            return int(prediction), float(confidence)
            
        except Exception as e:
            return 0, 0.0
    
    def batch_predict(self, df, min_confidence=0.6):
        """æ‰¹é‡é¢„æµ‹æ•´ä¸ªæ•°æ®é›†"""
        predictions = []
        confidences = []
        
        for i in range(len(df)):
            pred, conf = self.predict_signal(df, i)
            
            # åªä¿ç•™é«˜ç½®ä¿¡åº¦çš„äº¤æ˜“ä¿¡å·
            if pred != 0 and conf < min_confidence:
                pred = 0
            
            predictions.append(pred)
            confidences.append(conf)
        
        return predictions, confidences
    
    def evaluate_predictions(self, df, predictions, actual_labels=None):
        """è¯„ä¼°é¢„æµ‹ç»“æœ"""
        signal_names = {0: "ç­‰å¾…", 1: "åšå¤šå¼€ä»“", 2: "åšå¤šå¹³ä»“", 3: "åšç©ºå¼€ä»“", 4: "åšç©ºå¹³ä»“"}
        
        # ç»Ÿè®¡é¢„æµ‹ä¿¡å·
        pred_counts = {}
        for pred in predictions:
            pred_counts[pred] = pred_counts.get(pred, 0) + 1
        
        print("\né¢„æµ‹ä¿¡å·ç»Ÿè®¡:")
        for signal, count in sorted(pred_counts.items()):
            percentage = count / len(predictions) * 100
            print(f"  {signal_names.get(signal, f'æœªçŸ¥({signal})')}: {count} æ¬¡ ({percentage:.2f}%)")
        
        # å¦‚æœæœ‰å®é™…æ ‡ç­¾ï¼Œè®¡ç®—å‡†ç¡®ç‡
        if actual_labels is not None:
            trading_signals_pred = [p for p in predictions if p != 0]
            trading_signals_actual = [a for a in actual_labels if a != 0]
            
            if len(trading_signals_pred) > 0 and len(trading_signals_actual) > 0:
                # åªè¯„ä¼°äº¤æ˜“ä¿¡å·çš„å‡†ç¡®æ€§
                correct_trading = sum(1 for p, a in zip(predictions, actual_labels) 
                                    if p != 0 and p == a)
                total_trading_pred = sum(1 for p in predictions if p != 0)
                
                if total_trading_pred > 0:
                    trading_precision = correct_trading / total_trading_pred
                    print(f"\näº¤æ˜“ä¿¡å·ç²¾ç¡®ç‡: {trading_precision:.2%}")
                    print(f"æ­£ç¡®äº¤æ˜“ä¿¡å·: {correct_trading}/{total_trading_pred}")

def test_predictor():
    """æµ‹è¯•é¢„æµ‹å™¨"""
    predictor = TradingSignalPredictor()
    
    # æµ‹è¯•æ•°æ®æ–‡ä»¶
    test_files = [f for f in os.listdir("label") if f.endswith(".csv")][:3]
    
    for file_name in test_files:
        print(f"\n=== æµ‹è¯•æ–‡ä»¶: {file_name} ===")
        
        file_path = os.path.join("label", file_name)
        df = pd.read_csv(file_path)
        
        # æ‰¹é‡é¢„æµ‹
        predictions, confidences = predictor.batch_predict(df, min_confidence=0.7)
        
        # è¯„ä¼°ç»“æœ
        actual_labels = df['label'].tolist() if 'label' in df.columns else None
        predictor.evaluate_predictions(df, predictions, actual_labels)

if __name__ == "__main__":
    test_predictor()
'''
        
        predictor_file = os.path.join(self.base_dir, "trading_signal_predictor.py")
        with open(predictor_file, 'w', encoding='utf-8') as f:
            f.write(predictor_code)
        
        print(f"âœ“ äº¤æ˜“ä¿¡å·é¢„æµ‹å™¨å·²åˆ›å»º: {predictor_file}")
        return True
    
    def run_focused_improvement(self):
        """è¿è¡Œä¸“æ³¨çš„æ”¹è¿›æµç¨‹"""
        print("\n" + "="*60)
        print("äº¤æ˜“ä¿¡å·é¢„æµ‹ç³»ç»Ÿ - ä¸“æ³¨æ”¹è¿›æµç¨‹")
        print("ä¸“æ³¨äºæé«˜1-4äº¤æ˜“ä¿¡å·çš„é¢„æµ‹å‡†ç¡®æ€§")
        print("="*60)
        
        success_count = 0
        total_steps = 5
        
        # 1. åˆ†æäº¤æ˜“ä¿¡å·è´¨é‡
        try:
            signal_contexts = self.analyze_trading_signal_quality()
            success_count += 1
        except Exception as e:
            print(f"ä¿¡å·è´¨é‡åˆ†æå¤±è´¥: {e}")
        
        # 2. åˆ›å»ºäº¤æ˜“ä¸“ç”¨ç‰¹å¾
        try:
            X, y = self.create_trading_focused_features()
            if len(X) > 0:
                success_count += 1
            else:
                print("ç‰¹å¾åˆ›å»ºå¤±è´¥: æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®")
                return False
        except Exception as e:
            print(f"ç‰¹å¾åˆ›å»ºå¤±è´¥: {e}")
            return False
        
        # 3. è®­ç»ƒä¸“ç”¨æ¨¡å‹
        try:
            results = self.train_specialized_models(X, y)
            if results:
                success_count += 1
        except Exception as e:
            print(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        
        # 4. åˆ›å»ºé¢„æµ‹å™¨
        try:
            if self.create_trading_predictor():
                success_count += 1
        except Exception as e:
            print(f"é¢„æµ‹å™¨åˆ›å»ºå¤±è´¥: {e}")
        
        # 5. ç”Ÿæˆæ”¹è¿›æŠ¥å‘Š
        try:
            if self.generate_focused_report():
                success_count += 1
        except Exception as e:
            print(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        
        print(f"\n" + "="*60)
        print(f"ä¸“æ³¨æ”¹è¿›å®Œæˆ: {success_count}/{total_steps} æ­¥éª¤æˆåŠŸ")
        print("="*60)
        
        if success_count >= 4:
            print("\nğŸ‰ äº¤æ˜“ä¿¡å·é¢„æµ‹ç³»ç»Ÿæ”¹è¿›æˆåŠŸï¼")
            print("\nå…³é”®æ”¹è¿›:")
            print("1. âœ“ æ­£ç¡®ç†è§£0ä¿¡å·çš„å«ä¹‰ï¼ˆç­‰å¾…çŠ¶æ€ï¼‰")
            print("2. âœ“ ä¸“æ³¨äºæé«˜1-4äº¤æ˜“ä¿¡å·çš„é¢„æµ‹å‡†ç¡®æ€§")
            print("3. âœ“ åˆ›å»ºäº†å¢å¼ºçš„äº¤æ˜“ç‰¹å¾")
            print("4. âœ“ è®­ç»ƒäº†ä¸“é—¨çš„äº¤æ˜“ä¿¡å·æ¨¡å‹")
            print("5. âœ“ æä¾›äº†ä¸“ç”¨çš„é¢„æµ‹å™¨")
            
            print("\nä¸‹ä¸€æ­¥å»ºè®®:")
            print("1. è¿è¡Œ: python trading_signal_predictor.py")
            print("2. æµ‹è¯•æ–°æ¨¡å‹çš„äº¤æ˜“ä¿¡å·é¢„æµ‹æ•ˆæœ")
            print("3. ç›‘æ§å®é™…äº¤æ˜“ä¸­çš„ä¿¡å·è´¨é‡")
            print("4. æ ¹æ®å®é™…æ•ˆæœè¿›ä¸€æ­¥è°ƒä¼˜å‚æ•°")
        else:
            print(f"\nâš ï¸  æ”¹è¿›è¿‡ç¨‹ä¸­æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        
        return success_count >= 4
    
    def generate_focused_report(self):
        """ç”Ÿæˆä¸“æ³¨æ”¹è¿›æŠ¥å‘Š"""
        print(f"\n=== ç”Ÿæˆä¸“æ³¨æ”¹è¿›æŠ¥å‘Š ===")
        
        report = {
            'improvement_date': datetime.now().isoformat(),
            'improvement_philosophy': {
                'understanding': '0ä¿¡å·ä»£è¡¨ç­‰å¾…çŠ¶æ€ï¼Œè¿™æ˜¯æ­£å¸¸çš„äº¤æ˜“è¡Œä¸º',
                'focus': 'ä¸“æ³¨äºæé«˜1-4äº¤æ˜“ä¿¡å·çš„é¢„æµ‹å‡†ç¡®æ€§',
                'goal': 'å®ç°æ¯å¤©1-2æ¬¡é«˜è´¨é‡çš„å¼€ä»“å’Œå¹³ä»“ä¿¡å·'
            },
            'improvements_applied': [
                'é‡æ–°å®šä¹‰é—®é¢˜ï¼šä»ä¿¡å·å¹³è¡¡è½¬å‘ä¿¡å·è´¨é‡',
                'åˆ›å»ºäº¤æ˜“ä¸“ç”¨ç‰¹å¾å·¥ç¨‹',
                'è®­ç»ƒä¸“é—¨çš„äº¤æ˜“ä¿¡å·åˆ†ç±»æ¨¡å‹',
                'å®ç°é«˜ç½®ä¿¡åº¦ä¿¡å·è¿‡æ»¤',
                'å»ºç«‹äº¤æ˜“ä¿¡å·è´¨é‡è¯„ä¼°ä½“ç³»'
            ],
            'technical_enhancements': [
                'å¢å¼ºä»·æ ¼å˜åŒ–ç‰¹å¾ï¼ˆå¤šæ—¶é—´çª—å£ï¼‰',
                'æ·»åŠ æ³¢åŠ¨ç‡å’Œè¶‹åŠ¿å¼ºåº¦æŒ‡æ ‡',
                'å®ç°æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ï¼ˆç§»åŠ¨å¹³å‡ç­‰ï¼‰',
                'ä½¿ç”¨ç±»åˆ«å¹³è¡¡çš„éšæœºæ£®æ—æ¨¡å‹',
                'é›†æˆæ¢¯åº¦æå‡æ¨¡å‹æé«˜å‡†ç¡®æ€§'
            ],
            'expected_benefits': [
                'æé«˜äº¤æ˜“ä¿¡å·çš„é¢„æµ‹ç²¾ç¡®ç‡',
                'å‡å°‘å‡ä¿¡å·ï¼Œæé«˜ä¿¡å·å¯é æ€§',
                'æ›´å¥½åœ°æ•æ‰å¸‚åœºè½¬æŠ˜ç‚¹',
                'é€‚åº”è‚¡æŒ‡æœŸè´§äº¤æ˜“çš„å®é™…éœ€æ±‚'
            ],
            'evaluation_metrics': [
                'äº¤æ˜“ä¿¡å·ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰',
                'äº¤æ˜“ä¿¡å·å¬å›ç‡ï¼ˆRecallï¼‰',
                'F1å¾—åˆ†ï¼ˆå¹³è¡¡ç²¾ç¡®ç‡å’Œå¬å›ç‡ï¼‰',
                'å„ç±»ä¿¡å·çš„ç‹¬ç«‹æ€§èƒ½è¯„ä¼°'
            ],
            'next_steps': [
                'æµ‹è¯•æ–°é¢„æµ‹å™¨çš„å®é™…æ•ˆæœ',
                'æ”¶é›†æ›´å¤šå†å²æ•°æ®è¿›è¡ŒéªŒè¯',
                'æ ¹æ®å®é™…äº¤æ˜“ç»“æœè°ƒä¼˜æ¨¡å‹',
                'å»ºç«‹åœ¨çº¿å­¦ä¹ æœºåˆ¶æŒç»­æ”¹è¿›'
            ]
        }
        
        report_file = os.path.join(self.base_dir, "focused_improvement_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ ä¸“æ³¨æ”¹è¿›æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return True

if __name__ == "__main__":
    improver = FocusedImprovement()
    improver.run_focused_improvement()