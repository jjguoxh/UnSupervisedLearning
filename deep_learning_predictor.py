# -*- coding: utf-8 -*-
"""
æ·±åº¦å­¦ä¹ é¢„æµ‹å™¨
ä¸“ä¸ºè‚¡æŒ‡æœŸè´§çŸ­æœŸå‰§çƒˆæ³¢åŠ¨è®¾è®¡
ä½¿ç”¨Transformer + CNNæ¶æ„
"""

import pandas as pd
import numpy as np
import os
import glob
from collections import Counter
import talib
import warnings
warnings.filterwarnings('ignore')

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split
    TORCH_AVAILABLE = True
except ImportError:
    print("PyTorchæœªå®‰è£…ï¼Œå°†ä½¿ç”¨è½»é‡çº§ç¥ç»ç½‘ç»œå®ç°")
    TORCH_AVAILABLE = False

class VolatilityDataset(Dataset):
    """
    ä¸“ä¸ºæ³¢åŠ¨æ€§æ•°æ®è®¾è®¡çš„æ•°æ®é›†
    """
    def __init__(self, features, labels, sequence_length=20):
        self.features = features
        self.labels = labels
        self.sequence_length = sequence_length
        
    def __len__(self):
        return len(self.features) - self.sequence_length
    
    def __getitem__(self, idx):
        x = self.features[idx:idx + self.sequence_length]
        y = self.labels[idx + self.sequence_length]
        return torch.FloatTensor(x), torch.LongTensor([y])

class VolatilityTransformer(nn.Module):
    """
    ä¸“ä¸ºè‚¡æŒ‡æœŸè´§æ³¢åŠ¨æ€§è®¾è®¡çš„Transformeræ¨¡å‹
    """
    def __init__(self, input_dim, d_model=64, nhead=8, num_layers=3, num_classes=4):
        super(VolatilityTransformer, self).__init__()
        
        # è¾“å…¥æŠ•å½±å±‚
        self.input_projection = nn.Linear(input_dim, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = nn.Parameter(torch.randn(100, d_model))
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=nhead, 
            dim_feedforward=256,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # CNNå±‚ç”¨äºæ•æ‰å±€éƒ¨æ¨¡å¼
        self.conv1d = nn.Conv1d(d_model, 128, kernel_size=3, padding=1)
        self.conv_bn = nn.BatchNorm1d(128)
        
        # æ³¨æ„åŠ›æ± åŒ–
        self.attention_pool = nn.MultiheadAttention(128, 4, batch_first=True)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(32, num_classes)
        )
        
    def forward(self, x):
        batch_size, seq_len, _ = x.shape
        
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_encoding[:seq_len].unsqueeze(0)
        
        # Transformerç¼–ç 
        x = self.transformer(x)
        
        # CNNå¤„ç†
        x = x.transpose(1, 2)  # (batch, d_model, seq_len)
        x = torch.relu(self.conv_bn(self.conv1d(x)))
        x = x.transpose(1, 2)  # (batch, seq_len, 128)
        
        # æ³¨æ„åŠ›æ± åŒ–
        attn_output, _ = self.attention_pool(x, x, x)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = torch.mean(attn_output, dim=1)
        
        # åˆ†ç±»
        output = self.classifier(x)
        
        return output

class LightweightNN:
    """
    è½»é‡çº§ç¥ç»ç½‘ç»œå®ç°ï¼ˆä¸ä¾èµ–PyTorchï¼‰
    """
    def __init__(self, input_dim, hidden_dims=[64, 32], num_classes=4):
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.num_classes = num_classes
        self.weights = []
        self.biases = []
        
        # åˆå§‹åŒ–æƒé‡
        dims = [input_dim] + hidden_dims + [num_classes]
        for i in range(len(dims) - 1):
            w = np.random.randn(dims[i], dims[i+1]) * 0.1
            b = np.zeros(dims[i+1])
            self.weights.append(w)
            self.biases.append(b)
    
    def relu(self, x):
        return np.maximum(0, x)
    
    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)
    
    def forward(self, x):
        for i, (w, b) in enumerate(zip(self.weights, self.biases)):
            x = np.dot(x, w) + b
            if i < len(self.weights) - 1:  # ä¸åœ¨æœ€åä¸€å±‚ä½¿ç”¨æ¿€æ´»å‡½æ•°
                x = self.relu(x)
        return self.softmax(x)
    
    def train_simple(self, X, y, epochs=100, lr=0.01):
        """
        ç®€å•çš„è®­ç»ƒè¿‡ç¨‹
        """
        for epoch in range(epochs):
            # å‰å‘ä¼ æ’­
            predictions = self.forward(X)
            
            # è®¡ç®—æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰
            y_onehot = np.eye(self.num_classes)[y]
            loss = -np.mean(np.sum(y_onehot * np.log(predictions + 1e-8), axis=1))
            
            # ç®€å•çš„æ¢¯åº¦ä¸‹é™ï¼ˆä»…æ›´æ–°æœ€åä¸€å±‚ï¼‰
            if epoch % 20 == 0:
                print(f"Epoch {epoch}, Loss: {loss:.4f}")
            
            # åå‘ä¼ æ’­ï¼ˆç®€åŒ–ç‰ˆï¼‰
            grad = predictions - y_onehot
            self.weights[-1] -= lr * np.dot(X.T, grad) / len(X)
            self.biases[-1] -= lr * np.mean(grad, axis=0)

class DeepLearningPredictor:
    def __init__(self):
        self.models_dir = "./models_deep/"
        self.scaler = StandardScaler()
        self.model = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if TORCH_AVAILABLE else None
        
    def extract_advanced_features(self, df, window_size=20):
        """
        æå–é«˜çº§ç‰¹å¾ç”¨äºæ·±åº¦å­¦ä¹ 
        """
        features = []
        labels = []
        
        for i in range(window_size, len(df) - 1):
            window_data = df.iloc[i-window_size:i]
            prices = window_data['index_value'].values.astype(float)
            
            try:
                # åŸºç¡€æŠ€æœ¯æŒ‡æ ‡
                sma_5 = talib.SMA(prices, timeperiod=5)
                sma_10 = talib.SMA(prices, timeperiod=10)
                sma_20 = talib.SMA(prices, timeperiod=20)
                ema_5 = talib.EMA(prices, timeperiod=5)
                ema_10 = talib.EMA(prices, timeperiod=10)
                
                # åŠ¨é‡æŒ‡æ ‡
                rsi = talib.RSI(prices, timeperiod=14)
                macd, macd_signal, macd_hist = talib.MACD(prices)
                
                # æ³¢åŠ¨ç‡æŒ‡æ ‡
                atr = talib.ATR(prices, prices, prices, timeperiod=14)
                
                # ä»·æ ¼å˜åŒ–ç‡
                roc_5 = talib.ROC(prices, timeperiod=5)
                roc_10 = talib.ROC(prices, timeperiod=10)
                
                # å¸ƒæ—å¸¦
                bb_upper, bb_middle, bb_lower = talib.BBANDS(prices)
                
                # å¨å»‰æŒ‡æ ‡
                willr = talib.WILLR(prices, prices, prices, timeperiod=14)
                
                # éšæœºæŒ‡æ ‡
                slowk, slowd = talib.STOCH(prices, prices, prices)
                
                # ä»·æ ¼ä½ç½®ç‰¹å¾
                current_price = prices[-1]
                price_position = (current_price - np.min(prices)) / (np.max(prices) - np.min(prices) + 1e-8)
                
                # æ³¢åŠ¨æ€§ç‰¹å¾
                returns = np.diff(prices) / prices[:-1]
                volatility = np.std(returns)
                skewness = self.calculate_skewness(returns)
                kurtosis = self.calculate_kurtosis(returns)
                
                # è¶‹åŠ¿å¼ºåº¦
                trend_strength = abs(prices[-1] - prices[0]) / (np.std(prices) + 1e-8)
                
                # ç»„åˆç‰¹å¾å‘é‡
                feature_vector = [
                    # ä»·æ ¼ç›¸å…³
                    current_price / np.mean(prices),
                    price_position,
                    
                    # ç§»åŠ¨å¹³å‡
                    sma_5[-1] / current_price if not np.isnan(sma_5[-1]) else 1,
                    sma_10[-1] / current_price if not np.isnan(sma_10[-1]) else 1,
                    sma_20[-1] / current_price if not np.isnan(sma_20[-1]) else 1,
                    ema_5[-1] / current_price if not np.isnan(ema_5[-1]) else 1,
                    ema_10[-1] / current_price if not np.isnan(ema_10[-1]) else 1,
                    
                    # åŠ¨é‡æŒ‡æ ‡
                    rsi[-1] / 100 if not np.isnan(rsi[-1]) else 0.5,
                    macd[-1] if not np.isnan(macd[-1]) else 0,
                    macd_signal[-1] if not np.isnan(macd_signal[-1]) else 0,
                    macd_hist[-1] if not np.isnan(macd_hist[-1]) else 0,
                    
                    # æ³¢åŠ¨ç‡
                    volatility,
                    atr[-1] / current_price if not np.isnan(atr[-1]) else 0,
                    
                    # å˜åŒ–ç‡
                    roc_5[-1] / 100 if not np.isnan(roc_5[-1]) else 0,
                    roc_10[-1] / 100 if not np.isnan(roc_10[-1]) else 0,
                    
                    # å¸ƒæ—å¸¦
                    (current_price - bb_lower[-1]) / (bb_upper[-1] - bb_lower[-1] + 1e-8) if not np.isnan(bb_upper[-1]) else 0.5,
                    
                    # å…¶ä»–æŒ‡æ ‡
                    willr[-1] / -100 if not np.isnan(willr[-1]) else 0.5,
                    slowk[-1] / 100 if not np.isnan(slowk[-1]) else 0.5,
                    slowd[-1] / 100 if not np.isnan(slowd[-1]) else 0.5,
                    
                    # ç»Ÿè®¡ç‰¹å¾
                    skewness,
                    kurtosis,
                    trend_strength
                ]
                
                # ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯æœ‰æ•ˆæ•°å€¼
                feature_vector = [f if not np.isnan(f) and not np.isinf(f) else 0 for f in feature_vector]
                
                features.append(feature_vector)
                
                # æ ‡ç­¾
                next_label = df['label'].iloc[i + 1]
                if next_label in [1, 2, 3, 4]:
                    labels.append(next_label - 1)  # è½¬æ¢ä¸º0-3
                else:
                    labels.append(0)  # é»˜è®¤æ ‡ç­¾
                    
            except Exception as e:
                print(f"ç‰¹å¾æå–é”™è¯¯: {e}")
                continue
        
        return np.array(features), np.array(labels)
    
    def calculate_skewness(self, data):
        """è®¡ç®—ååº¦"""
        if len(data) < 3:
            return 0
        mean = np.mean(data)
        std = np.std(data)
        if std == 0:
            return 0
        return np.mean(((data - mean) / std) ** 3)
    
    def calculate_kurtosis(self, data):
        """è®¡ç®—å³°åº¦"""
        if len(data) < 4:
            return 0
        mean = np.mean(data)
        std = np.std(data)
        if std == 0:
            return 0
        return np.mean(((data - mean) / std) ** 4) - 3
    
    def prepare_training_data(self, data_files=None):
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®
        """
        if data_files is None:
            data_files = sorted(glob.glob("./label/*.csv"))[:10]  # ä½¿ç”¨å‰10ä¸ªæ–‡ä»¶è®­ç»ƒ
        
        all_features = []
        all_labels = []
        
        print(f"å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨ {len(data_files)} ä¸ªæ–‡ä»¶...")
        
        for file_path in data_files:
            try:
                df = pd.read_csv(file_path)
                features, labels = self.extract_advanced_features(df)
                
                if len(features) > 0:
                    all_features.extend(features)
                    all_labels.extend(labels)
                    print(f"æ–‡ä»¶ {os.path.basename(file_path)}: {len(features)} ä¸ªæ ·æœ¬")
                    
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                continue
        
        if len(all_features) == 0:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
        
        X = np.array(all_features)
        y = np.array(all_labels)
        
        print(f"\næ€»è®­ç»ƒæ ·æœ¬: {len(X)}")
        print(f"ç‰¹å¾ç»´åº¦: {X.shape[1]}")
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {Counter(y)}")
        
        return X, y
    
    def train_model(self, X, y, epochs=50, batch_size=32):
        """
        è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹
        """
        print("\nå¼€å§‹è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹...")
        
        # æ•°æ®æ ‡å‡†åŒ–
        X_scaled = self.scaler.fit_transform(X)
        
        # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
        X_train, X_val, y_train, y_val = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )
        
        if TORCH_AVAILABLE:
            return self.train_pytorch_model(X_train, X_val, y_train, y_val, epochs, batch_size)
        else:
            return self.train_lightweight_model(X_train, y_train)
    
    def train_pytorch_model(self, X_train, X_val, y_train, y_val, epochs, batch_size):
        """
        ä½¿ç”¨PyTorchè®­ç»ƒæ¨¡å‹
        """
        print("ä½¿ç”¨PyTorch Transformeræ¨¡å‹è®­ç»ƒ...")
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = VolatilityDataset(X_train, y_train)
        val_dataset = VolatilityDataset(X_val, y_val)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        # åˆ›å»ºæ¨¡å‹
        input_dim = X_train.shape[1]
        self.model = VolatilityTransformer(input_dim).to(self.device)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)
        criterion = nn.CrossEntropyLoss()
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)
        
        best_val_acc = 0
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0
            train_correct = 0
            train_total = 0
            
            for batch_x, batch_y in train_loader:
                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                batch_y = batch_y.squeeze()
                
                optimizer.zero_grad()
                outputs = self.model(batch_x)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                train_total += batch_y.size(0)
                train_correct += (predicted == batch_y).sum().item()
            
            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_loss = 0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for batch_x, batch_y in val_loader:
                    batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                    batch_y = batch_y.squeeze()
                    
                    outputs = self.model(batch_x)
                    loss = criterion(outputs, batch_y)
                    
                    val_loss += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    val_total += batch_y.size(0)
                    val_correct += (predicted == batch_y).sum().item()
            
            train_acc = train_correct / train_total
            val_acc = val_correct / val_total
            
            scheduler.step(val_loss)
            
            if epoch % 10 == 0:
                print(f"Epoch {epoch}: Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                os.makedirs(self.models_dir, exist_ok=True)
                torch.save(self.model.state_dict(), os.path.join(self.models_dir, "best_model.pth"))
        
        print(f"\nè®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
        return best_val_acc
    
    def train_lightweight_model(self, X_train, y_train):
        """
        è®­ç»ƒè½»é‡çº§æ¨¡å‹
        """
        print("ä½¿ç”¨è½»é‡çº§ç¥ç»ç½‘ç»œè®­ç»ƒ...")
        
        self.model = LightweightNN(X_train.shape[1])
        self.model.train_simple(X_train, y_train, epochs=100)
        
        # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡
        predictions = self.model.forward(X_train)
        predicted_labels = np.argmax(predictions, axis=1)
        accuracy = np.mean(predicted_labels == y_train)
        
        print(f"\nè®­ç»ƒå®Œæˆï¼è®­ç»ƒå‡†ç¡®ç‡: {accuracy:.4f}")
        return accuracy
    
    def predict_signals(self, test_files=None, n_files=3):
        """
        é¢„æµ‹ä¿¡å·
        """
        if test_files is None:
            label_files = sorted(glob.glob("./label/*.csv"))
            test_files = label_files[-n_files:]
        
        print(f"\nå¼€å§‹æ·±åº¦å­¦ä¹ é¢„æµ‹ï¼Œä½¿ç”¨ {len(test_files)} ä¸ªæ–‡ä»¶...")
        
        all_predictions = []
        
        for file_path in test_files:
            try:
                df = pd.read_csv(file_path)
                features, actual_labels = self.extract_advanced_features(df)
                
                if len(features) == 0:
                    continue
                
                # æ ‡å‡†åŒ–ç‰¹å¾
                features_scaled = self.scaler.transform(features)
                
                # é¢„æµ‹
                if TORCH_AVAILABLE and hasattr(self.model, 'eval'):
                    self.model.eval()
                    with torch.no_grad():
                        features_tensor = torch.FloatTensor(features_scaled).to(self.device)
                        outputs = self.model(features_tensor)
                        probabilities = torch.softmax(outputs, dim=1).cpu().numpy()
                        predicted_labels = np.argmax(probabilities, axis=1)
                else:
                    probabilities = self.model.forward(features_scaled)
                    predicted_labels = np.argmax(probabilities, axis=1)
                
                # è®°å½•é¢„æµ‹ç»“æœ
                for i, (pred, actual, prob) in enumerate(zip(predicted_labels, actual_labels, probabilities)):
                    confidence = np.max(prob)
                    is_correct = pred == actual
                    
                    prediction_detail = {
                        'file': os.path.basename(file_path),
                        'index': i,
                        'predicted': pred + 1,  # è½¬æ¢å›1-4
                        'actual': actual + 1,
                        'confidence': confidence,
                        'correct': is_correct
                    }
                    
                    all_predictions.append(prediction_detail)
                
                print(f"æ–‡ä»¶ {os.path.basename(file_path)}: {len(features)} ä¸ªé¢„æµ‹")
                
            except Exception as e:
                print(f"é¢„æµ‹æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                continue
        
        return all_predictions
    
    def evaluate_performance(self, predictions):
        """
        è¯„ä¼°æ€§èƒ½
        """
        if not predictions:
            print("æ²¡æœ‰é¢„æµ‹ç»“æœå¯è¯„ä¼°")
            return
        
        # æ•´ä½“ç»Ÿè®¡
        total_predictions = len(predictions)
        correct_predictions = sum(1 for p in predictions if p['correct'])
        overall_accuracy = correct_predictions / total_predictions
        
        # ä¿¡å·ç±»å‹ç»Ÿè®¡
        signal_stats = {1: [], 2: [], 3: [], 4: []}
        for p in predictions:
            signal_stats[p['predicted']].append(p['correct'])
        
        print(f"\n=== æ·±åº¦å­¦ä¹ æ¨¡å‹æ€§èƒ½è¯„ä¼° ===")
        print(f"æ€»é¢„æµ‹æ•°: {total_predictions}")
        print(f"æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.4f}")
        
        signal_names = {1: 'åšå¤šå¼€ä»“', 2: 'åšå¤šå¹³ä»“', 3: 'åšç©ºå¼€ä»“', 4: 'åšç©ºå¹³ä»“'}
        signal_diversity = 0
        
        print("\nå„ä¿¡å·ç±»å‹è¡¨ç°:")
        for signal_type, results in signal_stats.items():
            if results:
                accuracy = sum(results) / len(results)
                print(f"  {signal_names[signal_type]}: {len(results)}æ¬¡, å‡†ç¡®ç‡{accuracy:.4f}")
                signal_diversity += 1
        
        print(f"\nä¿¡å·å¤šæ ·æ€§: {signal_diversity} ç§")
        
        # ç½®ä¿¡åº¦åˆ†æ
        confidences = [p['confidence'] for p in predictions]
        avg_confidence = np.mean(confidences)
        print(f"å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.4f}")
        
        # ä¿å­˜ç»“æœ
        os.makedirs(self.models_dir, exist_ok=True)
        results_df = pd.DataFrame(predictions)
        results_df.to_csv(os.path.join(self.models_dir, "deep_learning_results.csv"), index=False)
        
        return overall_accuracy, signal_diversity
    
    def run_complete_training(self):
        """
        è¿è¡Œå®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹
        """
        print("=== æ·±åº¦å­¦ä¹ é¢„æµ‹å™¨è®­ç»ƒå¼€å§‹ ===")
        print(f"ä½¿ç”¨è®¾å¤‡: {'GPU' if self.device and self.device.type == 'cuda' else 'CPU'}")
        
        try:
            # å‡†å¤‡æ•°æ®
            X, y = self.prepare_training_data()
            
            # è®­ç»ƒæ¨¡å‹
            train_accuracy = self.train_model(X, y)
            
            # é¢„æµ‹æµ‹è¯•
            predictions = self.predict_signals()
            
            # è¯„ä¼°æ€§èƒ½
            test_accuracy, diversity = self.evaluate_performance(predictions)
            
            print(f"\n=== æœ€ç»ˆç»“æœ ===")
            print(f"è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.4f}")
            print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")
            print(f"ä¿¡å·å¤šæ ·æ€§: {diversity} ç§")
            
            # ä¸ä¹‹å‰ç»“æœå¯¹æ¯”
            print(f"\n=== æ”¹è¿›æ•ˆæœ ===")
            print(f"è§„åˆ™æ–¹æ³•å‡†ç¡®ç‡: 33.3%")
            print(f"æ·±åº¦å­¦ä¹ å‡†ç¡®ç‡: {test_accuracy:.1%}")
            
            if test_accuracy > 0.333:
                improvement = (test_accuracy - 0.333) / 0.333 * 100
                print(f"âœ… å‡†ç¡®ç‡æå‡: {improvement:.1f}%")
            else:
                print(f"âš ï¸  å‡†ç¡®ç‡éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
            
            return test_accuracy, diversity
            
        except Exception as e:
            print(f"è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
            return 0, 0

def main():
    """
    ä¸»å‡½æ•°
    """
    predictor = DeepLearningPredictor()
    accuracy, diversity = predictor.run_complete_training()
    
    print(f"\n" + "=" * 60)
    print(f"ğŸ¯ æ·±åº¦å­¦ä¹ é¢„æµ‹å™¨æ€»ç»“:")
    print(f"   æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"   ä¿¡å·å¤šæ ·æ€§: {diversity} ç§")
    
    if accuracy > 0.4:
        print(f"\nâœ… æ·±åº¦å­¦ä¹ æ–¹æ³•æˆåŠŸæå‡äº†é¢„æµ‹å‡†ç¡®ç‡ï¼")
        print(f"   â€¢ ä½¿ç”¨äº†Transformer + CNNæ¶æ„")
        print(f"   â€¢ ä¸“ä¸ºè‚¡æŒ‡æœŸè´§æ³¢åŠ¨æ€§è®¾è®¡")
        print(f"   â€¢ æå–äº†22ç»´é«˜çº§ç‰¹å¾")
    else:
        print(f"\nâš ï¸  æ·±åº¦å­¦ä¹ æ–¹æ³•éœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜")
        print(f"   â€¢ å¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒæ•°æ®")
        print(f"   â€¢ å¯ä»¥å°è¯•ä¸åŒçš„ç½‘ç»œæ¶æ„")
        print(f"   â€¢ éœ€è¦ä¼˜åŒ–è¶…å‚æ•°")

if __name__ == "__main__":
    main()