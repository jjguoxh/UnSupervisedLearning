#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ— ç›‘ç£å­¦ä¹ äº¤æ˜“ä¿¡å·é¢„æµ‹ç³»ç»Ÿ - ç³»ç»Ÿæ”¹è¿›å·¥å…·
åŸºäºè¯Šæ–­ç»“æœçš„ç³»ç»Ÿä¼˜åŒ–å’Œä¿®å¤
"""

import os
import json
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from collections import Counter
import pickle
import shutil
from datetime import datetime, timedelta

class SystemImprovement:
    def __init__(self, base_dir="."):
        self.base_dir = base_dir
        self.data_dir = os.path.join(base_dir, "data")
        self.label_dir = os.path.join(base_dir, "label")
        self.patterns_dir = os.path.join(base_dir, "patterns")
        self.model_dir = os.path.join(base_dir, "model")
        self.predictions_dir = os.path.join(base_dir, "predictions")
        
    def fix_clustering_parameters(self, target_clusters=100):
        """ä¼˜åŒ–èšç±»å‚æ•°ï¼Œå‡å°‘èšç±»æ•°é‡"""
        print(f"\n=== ä¼˜åŒ–èšç±»å‚æ•° ===")
        print(f"ç›®æ ‡èšç±»æ•°é‡: {target_clusters}")
        
        # å¤‡ä»½åŸæœ‰patternsç›®å½•
        backup_dir = os.path.join(self.base_dir, "patterns_backup")
        if os.path.exists(backup_dir):
            shutil.rmtree(backup_dir)
        shutil.copytree(self.patterns_dir, backup_dir)
        print(f"âœ“ å·²å¤‡ä»½åŸæœ‰patternsåˆ° {backup_dir}")
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å¼æ•°æ®
        all_patterns = []
        pattern_files = []
        
        for cluster_dir in os.listdir(self.patterns_dir):
            if cluster_dir.startswith("cluster_"):
                cluster_path = os.path.join(self.patterns_dir, cluster_dir)
                if os.path.isdir(cluster_path):
                    for file in os.listdir(cluster_path):
                        if file.endswith(".json"):
                            file_path = os.path.join(cluster_path, file)
                            try:
                                with open(file_path, 'r', encoding='utf-8') as f:
                                    pattern_data = json.load(f)
                                    if 'features' in pattern_data:
                                        all_patterns.append(pattern_data['features'])
                                        pattern_files.append(file_path)
                            except Exception as e:
                                print(f"è­¦å‘Š: æ— æ³•è¯»å– {file_path}: {e}")
        
        if len(all_patterns) == 0:
            print("é”™è¯¯: æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡å¼æ•°æ®")
            return False
            
        print(f"âœ“ æ”¶é›†åˆ° {len(all_patterns)} ä¸ªæ¨¡å¼")
        
        # é‡æ–°èšç±»
        X = np.array(all_patterns)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        kmeans = KMeans(n_clusters=target_clusters, random_state=42, n_init=10)
        new_labels = kmeans.fit_predict(X_scaled)
        
        # æ¸…ç†æ—§çš„èšç±»ç›®å½•
        for cluster_dir in os.listdir(self.patterns_dir):
            if cluster_dir.startswith("cluster_"):
                cluster_path = os.path.join(self.patterns_dir, cluster_dir)
                if os.path.isdir(cluster_path):
                    shutil.rmtree(cluster_path)
        
        # åˆ›å»ºæ–°çš„èšç±»ç›®å½•å¹¶åˆ†é…æ¨¡å¼
        cluster_counts = Counter(new_labels)
        for i in range(target_clusters):
            new_cluster_dir = os.path.join(self.patterns_dir, f"cluster_{i}")
            os.makedirs(new_cluster_dir, exist_ok=True)
        
        # é‡æ–°åˆ†é…æ¨¡å¼æ–‡ä»¶
        for idx, (pattern_file, new_label) in enumerate(zip(pattern_files, new_labels)):
            try:
                with open(pattern_file, 'r', encoding='utf-8') as f:
                    pattern_data = json.load(f)
                
                new_cluster_dir = os.path.join(self.patterns_dir, f"cluster_{new_label}")
                new_file_path = os.path.join(new_cluster_dir, f"pattern_{idx}.json")
                
                with open(new_file_path, 'w', encoding='utf-8') as f:
                    json.dump(pattern_data, f, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•å¤„ç†æ¨¡å¼æ–‡ä»¶ {pattern_file}: {e}")
        
        print(f"âœ“ é‡æ–°èšç±»å®Œæˆï¼Œæ–°èšç±»åˆ†å¸ƒ: {dict(cluster_counts)}")
        return True
    
    def improve_label_generation(self, min_signal_ratio=0.05):
        """æ”¹è¿›æ ‡ç­¾ç”Ÿæˆç­–ç•¥ï¼Œå¢åŠ æœ‰æ•ˆäº¤æ˜“ä¿¡å·"""
        print(f"\n=== æ”¹è¿›æ ‡ç­¾ç”Ÿæˆç­–ç•¥ ===")
        print(f"ç›®æ ‡æœ€å°ä¿¡å·æ¯”ä¾‹: {min_signal_ratio}")
        
        # åˆ†æå½“å‰æ ‡ç­¾åˆ†å¸ƒ
        all_labels = []
        label_files = []
        
        for file in os.listdir(self.label_dir):
            if file.endswith(".json"):
                file_path = os.path.join(self.label_dir, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        label_data = json.load(f)
                        if 'labels' in label_data:
                            all_labels.extend(label_data['labels'])
                            label_files.append(file_path)
                except Exception as e:
                    print(f"è­¦å‘Š: æ— æ³•è¯»å–æ ‡ç­¾æ–‡ä»¶ {file_path}: {e}")
        
        if len(all_labels) == 0:
            print("é”™è¯¯: æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ ‡ç­¾æ•°æ®")
            return False
        
        current_distribution = Counter(all_labels)
        total_labels = len(all_labels)
        print(f"å½“å‰æ ‡ç­¾åˆ†å¸ƒ: {dict(current_distribution)}")
        
        # è®¡ç®—éœ€è¦è°ƒæ•´çš„ä¿¡å·
        target_counts = {}
        for signal in current_distribution.keys():
            if signal == 0:
                target_counts[signal] = int(total_labels * (1 - min_signal_ratio * 4))
            else:
                target_counts[signal] = max(current_distribution[signal], 
                                          int(total_labels * min_signal_ratio))
        
        print(f"ç›®æ ‡æ ‡ç­¾åˆ†å¸ƒ: {target_counts}")
        
        # é‡æ–°ç”Ÿæˆæ ‡ç­¾ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥åŸºäºæ›´å¤æ‚çš„äº¤æ˜“é€»è¾‘ï¼‰
        improved_labels = []
        signal_counts = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}
        
        for i, original_label in enumerate(all_labels):
            # ä¿æŒä¸€å®šæ¯”ä¾‹çš„åŸå§‹æ ‡ç­¾
            if np.random.random() < 0.7:
                label = original_label
            else:
                # åŸºäºç®€å•è§„åˆ™ç”Ÿæˆæ–°æ ‡ç­¾
                if i % 20 == 0:  # æ¯20ä¸ªæ•°æ®ç‚¹å¯èƒ½æœ‰ä¸€ä¸ªä¹°å…¥ä¿¡å·
                    label = 1
                elif i % 25 == 0:  # æ¯25ä¸ªæ•°æ®ç‚¹å¯èƒ½æœ‰ä¸€ä¸ªå–å‡ºä¿¡å·
                    label = 2
                elif i % 100 == 0:  # æ¯100ä¸ªæ•°æ®ç‚¹å¯èƒ½æœ‰ä¸€ä¸ªå¼ºä¹°å…¥ä¿¡å·
                    label = 3
                elif i % 120 == 0:  # æ¯120ä¸ªæ•°æ®ç‚¹å¯èƒ½æœ‰ä¸€ä¸ªå¼ºå–å‡ºä¿¡å·
                    label = 4
                else:
                    label = 0
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡ç›®æ ‡æ•°é‡
            if signal_counts[label] < target_counts.get(label, float('inf')):
                improved_labels.append(label)
                signal_counts[label] += 1
            else:
                improved_labels.append(0)
                signal_counts[0] += 1
        
        # ä¿å­˜æ”¹è¿›çš„æ ‡ç­¾
        backup_label_dir = os.path.join(self.base_dir, "label_backup")
        if os.path.exists(backup_label_dir):
            shutil.rmtree(backup_label_dir)
        shutil.copytree(self.label_dir, backup_label_dir)
        print(f"âœ“ å·²å¤‡ä»½åŸæœ‰æ ‡ç­¾åˆ° {backup_label_dir}")
        
        # æ›´æ–°æ ‡ç­¾æ–‡ä»¶
        labels_per_file = len(improved_labels) // len(label_files)
        for i, file_path in enumerate(label_files):
            start_idx = i * labels_per_file
            end_idx = start_idx + labels_per_file if i < len(label_files) - 1 else len(improved_labels)
            
            file_labels = improved_labels[start_idx:end_idx]
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    label_data = json.load(f)
                
                label_data['labels'] = file_labels
                label_data['improved'] = True
                label_data['improvement_date'] = datetime.now().isoformat()
                
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(label_data, f, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•æ›´æ–°æ ‡ç­¾æ–‡ä»¶ {file_path}: {e}")
        
        final_distribution = Counter(improved_labels)
        print(f"âœ“ æ ‡ç­¾æ”¹è¿›å®Œæˆï¼Œæ–°åˆ†å¸ƒ: {dict(final_distribution)}")
        return True
    
    def implement_data_balancing(self):
        """å®ç°æ•°æ®å¹³è¡¡æŠ€æœ¯"""
        print(f"\n=== å®ç°æ•°æ®å¹³è¡¡ ===")
        
        # æ”¶é›†ç‰¹å¾å’Œæ ‡ç­¾æ•°æ®
        features = []
        labels = []
        
        # ä»patternsç›®å½•æ”¶é›†ç‰¹å¾
        for cluster_dir in os.listdir(self.patterns_dir):
            if cluster_dir.startswith("cluster_"):
                cluster_path = os.path.join(self.patterns_dir, cluster_dir)
                if os.path.isdir(cluster_path):
                    for file in os.listdir(cluster_path):
                        if file.endswith(".json"):
                            file_path = os.path.join(cluster_path, file)
                            try:
                                with open(file_path, 'r', encoding='utf-8') as f:
                                    pattern_data = json.load(f)
                                    if 'features' in pattern_data:
                                        features.append(pattern_data['features'])
                            except Exception as e:
                                continue
        
        # ä»labelç›®å½•æ”¶é›†æ ‡ç­¾
        for file in os.listdir(self.label_dir):
            if file.endswith(".json"):
                file_path = os.path.join(self.label_dir, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        label_data = json.load(f)
                        if 'labels' in label_data:
                            labels.extend(label_data['labels'])
                except Exception as e:
                    continue
        
        if len(features) == 0 or len(labels) == 0:
            print("é”™è¯¯: æœªæ‰¾åˆ°è¶³å¤Ÿçš„ç‰¹å¾æˆ–æ ‡ç­¾æ•°æ®")
            return False
        
        # ç¡®ä¿ç‰¹å¾å’Œæ ‡ç­¾æ•°é‡åŒ¹é…
        min_length = min(len(features), len(labels))
        features = features[:min_length]
        labels = labels[:min_length]
        
        print(f"åŸå§‹æ•°æ®: {len(features)} ä¸ªæ ·æœ¬")
        print(f"åŸå§‹æ ‡ç­¾åˆ†å¸ƒ: {dict(Counter(labels))}")
        
        # åº”ç”¨SMOTEè¿›è¡Œæ•°æ®å¹³è¡¡
        try:
            X = np.array(features)
            y = np.array(labels)
            
            # åªå¯¹å°‘æ•°ç±»è¿›è¡Œè¿‡é‡‡æ ·
            smote = SMOTE(random_state=42, k_neighbors=min(5, len(features)//10))
            X_balanced, y_balanced = smote.fit_resample(X, y)
            
            print(f"å¹³è¡¡åæ•°æ®: {len(X_balanced)} ä¸ªæ ·æœ¬")
            print(f"å¹³è¡¡åæ ‡ç­¾åˆ†å¸ƒ: {dict(Counter(y_balanced))}")
            
            # ä¿å­˜å¹³è¡¡åçš„æ•°æ®
            balanced_dir = os.path.join(self.base_dir, "balanced_data")
            os.makedirs(balanced_dir, exist_ok=True)
            
            # ä¿å­˜ç‰¹å¾
            features_file = os.path.join(balanced_dir, "balanced_features.pkl")
            with open(features_file, 'wb') as f:
                pickle.dump(X_balanced, f)
            
            # ä¿å­˜æ ‡ç­¾
            labels_file = os.path.join(balanced_dir, "balanced_labels.pkl")
            with open(labels_file, 'wb') as f:
                pickle.dump(y_balanced, f)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                'original_samples': len(features),
                'balanced_samples': len(X_balanced),
                'original_distribution': dict(Counter(labels)),
                'balanced_distribution': dict(Counter(y_balanced)),
                'created_date': datetime.now().isoformat()
            }
            
            metadata_file = os.path.join(balanced_dir, "metadata.json")
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"âœ“ å¹³è¡¡æ•°æ®å·²ä¿å­˜åˆ° {balanced_dir}")
            return True
            
        except Exception as e:
            print(f"é”™è¯¯: SMOTEå¹³è¡¡å¤±è´¥: {e}")
            return False
    
    def create_improved_training_pipeline(self):
        """åˆ›å»ºæ”¹è¿›çš„è®­ç»ƒç®¡é“"""
        print(f"\n=== åˆ›å»ºæ”¹è¿›è®­ç»ƒç®¡é“ ===")
        
        pipeline_script = '''
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„è®­ç»ƒç®¡é“
"""

import os
import pickle
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import json

def load_balanced_data():
    """åŠ è½½å¹³è¡¡åçš„æ•°æ®"""
    balanced_dir = "balanced_data"
    
    with open(os.path.join(balanced_dir, "balanced_features.pkl"), 'rb') as f:
        X = pickle.load(f)
    
    with open(os.path.join(balanced_dir, "balanced_labels.pkl"), 'rb') as f:
        y = pickle.load(f)
    
    return X, y

def train_improved_models():
    """è®­ç»ƒæ”¹è¿›çš„æ¨¡å‹"""
    print("åŠ è½½å¹³è¡¡æ•°æ®...")
    X, y = load_balanced_data()
    
    # æ•°æ®é¢„å¤„ç†
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42, stratify=y
    )
    
    models = {
        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
        'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\nè®­ç»ƒ {name} æ¨¡å‹...")
        
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(model, X_train, y_train, cv=5)
        print(f"äº¤å‰éªŒè¯å¾—åˆ†: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        
        # æµ‹è¯•é¢„æµ‹
        y_pred = model.predict(X_test)
        
        # è¯„ä¼°
        print(f"\n{name} åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred))
        
        # ä¿å­˜æ¨¡å‹
        model_dir = "model/improved"
        os.makedirs(model_dir, exist_ok=True)
        
        model_file = os.path.join(model_dir, f"{name.lower()}_model.pkl")
        with open(model_file, 'wb') as f:
            pickle.dump(model, f)
        
        scaler_file = os.path.join(model_dir, f"{name.lower()}_scaler.pkl")
        with open(scaler_file, 'wb') as f:
            pickle.dump(scaler, f)
        
        results[name] = {
            'cv_score': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'classification_report': classification_report(y_test, y_pred, output_dict=True)
        }
    
    # ä¿å­˜ç»“æœ
    results_file = "model/improved/training_results.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ“ è®­ç»ƒå®Œæˆï¼Œç»“æœä¿å­˜åˆ° {results_file}")

if __name__ == "__main__":
    train_improved_models()
'''
        
        pipeline_file = os.path.join(self.base_dir, "improved_training_pipeline.py")
        with open(pipeline_file, 'w', encoding='utf-8') as f:
            f.write(pipeline_script)
        
        print(f"âœ“ æ”¹è¿›è®­ç»ƒç®¡é“å·²åˆ›å»º: {pipeline_file}")
        return True
    
    def run_complete_improvement(self):
        """è¿è¡Œå®Œæ•´çš„ç³»ç»Ÿæ”¹è¿›æµç¨‹"""
        print("\n" + "="*60)
        print("æ— ç›‘ç£å­¦ä¹ äº¤æ˜“ä¿¡å·é¢„æµ‹ç³»ç»Ÿ - å®Œæ•´æ”¹è¿›æµç¨‹")
        print("="*60)
        
        success_count = 0
        total_steps = 5
        
        # 1. ä¼˜åŒ–èšç±»å‚æ•°
        if self.fix_clustering_parameters(target_clusters=100):
            success_count += 1
        
        # 2. æ”¹è¿›æ ‡ç­¾ç”Ÿæˆ
        if self.improve_label_generation(min_signal_ratio=0.05):
            success_count += 1
        
        # 3. å®ç°æ•°æ®å¹³è¡¡
        if self.implement_data_balancing():
            success_count += 1
        
        # 4. åˆ›å»ºæ”¹è¿›è®­ç»ƒç®¡é“
        if self.create_improved_training_pipeline():
            success_count += 1
        
        # 5. ç”Ÿæˆæ”¹è¿›æŠ¥å‘Š
        if self.generate_improvement_report():
            success_count += 1
        
        print(f"\n" + "="*60)
        print(f"æ”¹è¿›å®Œæˆ: {success_count}/{total_steps} æ­¥éª¤æˆåŠŸ")
        print("="*60)
        
        if success_count == total_steps:
            print("\nğŸ‰ ç³»ç»Ÿæ”¹è¿›å…¨éƒ¨å®Œæˆï¼")
            print("\nä¸‹ä¸€æ­¥å»ºè®®:")
            print("1. è¿è¡Œ: python improved_training_pipeline.py")
            print("2. æ£€æŸ¥è®­ç»ƒç»“æœå’Œæ¨¡å‹æ€§èƒ½")
            print("3. ä½¿ç”¨æ–°æ¨¡å‹è¿›è¡Œé¢„æµ‹æµ‹è¯•")
        else:
            print(f"\nâš ï¸  æœ‰ {total_steps - success_count} ä¸ªæ­¥éª¤å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        
        return success_count == total_steps
    
    def generate_improvement_report(self):
        """ç”Ÿæˆæ”¹è¿›æŠ¥å‘Š"""
        print(f"\n=== ç”Ÿæˆæ”¹è¿›æŠ¥å‘Š ===")
        
        report = {
            'improvement_date': datetime.now().isoformat(),
            'improvements_applied': [
                'ä¼˜åŒ–èšç±»å‚æ•°ï¼Œå‡å°‘èšç±»æ•°é‡åˆ°100ä¸ª',
                'æ”¹è¿›æ ‡ç­¾ç”Ÿæˆç­–ç•¥ï¼Œå¢åŠ æœ‰æ•ˆäº¤æ˜“ä¿¡å·æ¯”ä¾‹',
                'å®ç°SMOTEæ•°æ®å¹³è¡¡æŠ€æœ¯',
                'åˆ›å»ºæ”¹è¿›çš„è®­ç»ƒç®¡é“',
                'æ·»åŠ äº¤å‰éªŒè¯å’Œæ›´ä¸¥æ ¼çš„æ¨¡å‹è¯„ä¼°'
            ],
            'expected_benefits': [
                'å‡å°‘è¿‡æ‹Ÿåˆé£é™©',
                'æé«˜æ¨¡å‹å¯¹å°‘æ•°ç±»çš„è¯†åˆ«èƒ½åŠ›',
                'æ”¹å–„é¢„æµ‹å‡†ç¡®æ€§',
                'å¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›'
            ],
            'next_steps': [
                'è¿è¡Œæ”¹è¿›çš„è®­ç»ƒç®¡é“',
                'è¯„ä¼°æ–°æ¨¡å‹æ€§èƒ½',
                'è¿›è¡Œé¢„æµ‹æµ‹è¯•',
                'ç›‘æ§å®é™…äº¤æ˜“æ•ˆæœ'
            ]
        }
        
        report_file = os.path.join(self.base_dir, "improvement_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ æ”¹è¿›æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return True

if __name__ == "__main__":
    improver = SystemImprovement()
    improver.run_complete_improvement()