# -*- coding: utf-8 -*-
"""
è‚¡æŒ‡æœŸè´§äº¤æ˜“ä¿¡å·é¢„æµ‹ä¸å¯è§†åŒ–
å¯¹resultç›®å½•ä¸­çš„CSVæ–‡ä»¶è¿›è¡Œé¢„æµ‹å¹¶ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
"""

import pandas as pd
import numpy as np
import os
import glob
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

try:
    import torch
    import torch.nn as nn
    from sklearn.preprocessing import StandardScaler
    import talib
    TORCH_AVAILABLE = True
except ImportError:
    print("ç¼ºå°‘å¿…è¦çš„åº“ï¼Œè¯·å®‰è£…: pip install torch scikit-learn TA-Lib")
    TORCH_AVAILABLE = False

class ImprovedVolatilityNet(nn.Module):
    """
    æ”¹è¿›çš„æ³¢åŠ¨æ€§é¢„æµ‹ç½‘ç»œï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
    """
    def __init__(self, input_dim, num_classes=4):
        super(ImprovedVolatilityNet, self).__init__()
        
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        self.classifier = nn.Linear(32, num_classes)
        
    def forward(self, x):
        features = self.feature_extractor(x)
        output = self.classifier(features)
        return output

class TradingSignalPredictor:
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if TORCH_AVAILABLE else None
        self.signal_names = {
            1: 'åšå¤šå¼€ä»“',
            2: 'åšå¤šå¹³ä»“', 
            3: 'åšç©ºå¼€ä»“',
            4: 'åšç©ºå¹³ä»“'
        }
        self.signal_colors = {
            1: 'green',    # åšå¤šå¼€ä»“ - ç»¿è‰²ä¸Šä¸‰è§’
            2: 'green',    # åšå¤šå¹³ä»“ - ç»¿è‰²ä¸‹ä¸‰è§’
            3: 'red',      # åšç©ºå¼€ä»“ - çº¢è‰²ä¸‹ä¸‰è§’
            4: 'red'       # åšç©ºå¹³ä»“ - çº¢è‰²ä¸Šä¸‰è§’
        }
        self.signal_markers = {
            1: '^',        # åšå¤šå¼€ä»“ - ä¸Šä¸‰è§’
            2: 'v',        # åšå¤šå¹³ä»“ - ä¸‹ä¸‰è§’
            3: 'v',        # åšç©ºå¼€ä»“ - ä¸‹ä¸‰è§’
            4: '^'         # åšç©ºå¹³ä»“ - ä¸Šä¸‰è§’
        }
        
    def load_model(self):
        """
        åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        """
        model_path = "./models_deep_fixed/best_model.pth"
        scaler_path = "./models_deep_fixed/scaler.pkl"
        
        if not os.path.exists(model_path):
            print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            print("è¯·å…ˆè¿è¡Œ fixed_deep_learning_predictor.py è®­ç»ƒæ¨¡å‹")
            return False
            
        try:
            # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ˆéœ€è¦çŸ¥é“è¾“å…¥ç»´åº¦ï¼‰
            input_dim = 20  # æ ¹æ®ç‰¹å¾æå–å‡½æ•°ç¡®å®š
            self.model = ImprovedVolatilityNet(input_dim).to(self.device)
            
            # åŠ è½½æ¨¡å‹æƒé‡
            self.model.load_state_dict(torch.load(model_path, map_location=self.device))
            self.model.eval()
            
            # åŠ è½½æ ‡å‡†åŒ–å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            try:
                import pickle
                if os.path.exists(scaler_path):
                    with open(scaler_path, 'rb') as f:
                        self.scaler = pickle.load(f)
                else:
                    print("è­¦å‘Š: æœªæ‰¾åˆ°æ ‡å‡†åŒ–å™¨æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤æ ‡å‡†åŒ–å™¨")
            except:
                print("è­¦å‘Š: åŠ è½½æ ‡å‡†åŒ–å™¨å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ ‡å‡†åŒ–å™¨")
            
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def extract_features(self, df):
        """
        æå–ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
        """
        features = []
        indices = []
        
        if len(df) < 50:
            return np.array([]), []
        
        for i in range(30, len(df)):
            try:
                window_data = df.iloc[i-30:i]
                prices = window_data['index_value'].values.astype(float)
                
                if len(prices) < 30:
                    continue
                
                # åŸºç¡€ä»·æ ¼ç‰¹å¾
                current_price = prices[-1]
                price_mean = np.mean(prices)
                price_std = np.std(prices)
                
                if price_std == 0:
                    continue
                
                # æ ‡å‡†åŒ–ä»·æ ¼å˜åŒ–
                returns = np.diff(prices) / prices[:-1]
                
                # æŠ€æœ¯æŒ‡æ ‡
                sma_5 = talib.SMA(prices, timeperiod=5)
                sma_10 = talib.SMA(prices, timeperiod=10)
                sma_20 = talib.SMA(prices, timeperiod=20)
                
                rsi = talib.RSI(prices, timeperiod=14)
                macd, macd_signal, macd_hist = talib.MACD(prices)
                
                # æ³¢åŠ¨ç‡æŒ‡æ ‡
                volatility_5 = np.std(returns[-5:]) if len(returns) >= 5 else 0
                volatility_10 = np.std(returns[-10:]) if len(returns) >= 10 else 0
                volatility_20 = np.std(returns[-20:]) if len(returns) >= 20 else 0
                
                # åŠ¨é‡æŒ‡æ ‡
                momentum_5 = (current_price - prices[-6]) / prices[-6] if len(prices) >= 6 else 0
                momentum_10 = (current_price - prices[-11]) / prices[-11] if len(prices) >= 11 else 0
                
                # è¶‹åŠ¿æŒ‡æ ‡
                trend_5 = 1 if sma_5[-1] > sma_5[-2] else 0 if not np.isnan(sma_5[-1]) and not np.isnan(sma_5[-2]) else 0.5
                trend_10 = 1 if sma_10[-1] > sma_10[-2] else 0 if not np.isnan(sma_10[-1]) and not np.isnan(sma_10[-2]) else 0.5
                
                # ç›¸å¯¹ä½ç½®
                price_position = (current_price - np.min(prices)) / (np.max(prices) - np.min(prices))
                
                # ç»„åˆç‰¹å¾
                feature_vector = [
                    (current_price - price_mean) / price_std,
                    price_position,
                    current_price / sma_5[-1] - 1 if not np.isnan(sma_5[-1]) and sma_5[-1] > 0 else 0,
                    current_price / sma_10[-1] - 1 if not np.isnan(sma_10[-1]) and sma_10[-1] > 0 else 0,
                    current_price / sma_20[-1] - 1 if not np.isnan(sma_20[-1]) and sma_20[-1] > 0 else 0,
                    (rsi[-1] - 50) / 50 if not np.isnan(rsi[-1]) else 0,
                    macd[-1] / price_std if not np.isnan(macd[-1]) else 0,
                    macd_signal[-1] / price_std if not np.isnan(macd_signal[-1]) else 0,
                    macd_hist[-1] / price_std if not np.isnan(macd_hist[-1]) else 0,
                    volatility_5,
                    volatility_10,
                    volatility_20,
                    momentum_5,
                    momentum_10,
                    trend_5,
                    trend_10,
                    np.mean(returns[-5:]) if len(returns) >= 5 else 0,
                    np.mean(returns[-10:]) if len(returns) >= 10 else 0,
                    1 if current_price == np.max(prices[-10:]) else 0,
                    1 if current_price == np.min(prices[-10:]) else 0
                ]
                
                # æ£€æŸ¥ç‰¹å¾æœ‰æ•ˆæ€§
                if any(np.isnan(f) or np.isinf(f) for f in feature_vector):
                    continue
                
                features.append(feature_vector)
                indices.append(i)
                
            except Exception as e:
                continue
        
        return np.array(features), indices
    
    def filter_trading_signals(self, raw_signals, confidence_threshold=0.7, max_daily_trades=3):
        """
        è¿‡æ»¤äº¤æ˜“ä¿¡å·ï¼Œåº”ç”¨äº¤æ˜“é€»è¾‘çº¦æŸå’Œè´¨é‡æ§åˆ¶
        çº¦æŸè§„åˆ™ï¼š
        1. åœ¨ä¸€ä¸ªæ–¹å‘å¼€ä»“åï¼Œå¿…é¡»ç­‰è¯¥æ–¹å‘å¹³ä»“æ‰èƒ½å‡ºç°åæ–¹å‘å¼€ä»“ä¿¡å·
        2. åŒä¸€æ–¹å‘çš„é‡å¤å¼€ä»“ä¿¡å·åªä¿ç•™ç¬¬ä¸€ä¸ª
        3. åªä¿ç•™é«˜ç½®ä¿¡åº¦ä¿¡å·ï¼ˆç½®ä¿¡åº¦ >= confidence_thresholdï¼‰
        4. æ¯æ—¥æœ€å¤šå…è®¸max_daily_tradesç¬”å¼€ä»“äº¤æ˜“
        """
        if not raw_signals:
            return []
        
        # é¦–å…ˆæŒ‰ç½®ä¿¡åº¦è¿‡æ»¤
        high_confidence_signals = []
        for signal in raw_signals:
            if signal['confidence'] >= confidence_threshold:
                high_confidence_signals.append(signal)
        
        print(f"ç½®ä¿¡åº¦è¿‡æ»¤: {len(raw_signals)} -> {len(high_confidence_signals)} (é˜ˆå€¼: {confidence_threshold})")
        
        if not high_confidence_signals:
            return []
        
        filtered_signals = []
        position_state = 0  # 0: æ— ä»“ä½, 1: å¤šå¤´ä»“ä½, -1: ç©ºå¤´ä»“ä½
        daily_open_count = 0  # å½“æ—¥å¼€ä»“æ¬¡æ•°è®¡æ•°
        
        for signal in high_confidence_signals:
            label = signal['label']
            should_keep = False
            
            if label == 1:  # åšå¤šå¼€ä»“
                if position_state == 0 and daily_open_count < max_daily_trades:  # æ— ä»“ä½ä¸”æœªè¶…è¿‡æ¯æ—¥é™åˆ¶
                    should_keep = True
                    position_state = 1
                    daily_open_count += 1
                # å¦‚æœå·²æœ‰å¤šå¤´ä»“ä½æˆ–ç©ºå¤´ä»“ä½ï¼Œæˆ–è¶…è¿‡æ¯æ—¥é™åˆ¶ï¼Œå¿½ç•¥å¼€ä»“ä¿¡å·
                
            elif label == 2:  # åšå¤šå¹³ä»“
                if position_state == 1:  # æœ‰å¤šå¤´ä»“ä½æ—¶å¯ä»¥å¹³ä»“
                    should_keep = True
                    position_state = 0
                # å¦‚æœæ— å¤šå¤´ä»“ä½ï¼Œå¿½ç•¥å¹³ä»“ä¿¡å·
                
            elif label == 3:  # åšç©ºå¼€ä»“
                if position_state == 0 and daily_open_count < max_daily_trades:  # æ— ä»“ä½ä¸”æœªè¶…è¿‡æ¯æ—¥é™åˆ¶
                    should_keep = True
                    position_state = -1
                    daily_open_count += 1
                # å¦‚æœå·²æœ‰ç©ºå¤´ä»“ä½æˆ–å¤šå¤´ä»“ä½ï¼Œæˆ–è¶…è¿‡æ¯æ—¥é™åˆ¶ï¼Œå¿½ç•¥å¼€ä»“ä¿¡å·
                
            elif label == 4:  # åšç©ºå¹³ä»“
                if position_state == -1:  # æœ‰ç©ºå¤´ä»“ä½æ—¶å¯ä»¥å¹³ä»“
                    should_keep = True
                    position_state = 0
                # å¦‚æœæ— ç©ºå¤´ä»“ä½ï¼Œå¿½ç•¥å¹³ä»“ä¿¡å·
            
            if should_keep:
                filtered_signals.append(signal)
        
        print(f"äº¤æ˜“é€»è¾‘è¿‡æ»¤: {len(high_confidence_signals)} -> {len(filtered_signals)} (æ¯æ—¥å¼€ä»“é™åˆ¶: {max_daily_trades})")
        return filtered_signals
    
    def predict_signals(self, df):
        """
        é¢„æµ‹äº¤æ˜“ä¿¡å·ï¼ˆåº”ç”¨è¿‡æ»¤é€»è¾‘ï¼‰
        """
        features, indices = self.extract_features(df)
        
        if len(features) == 0:
            return [], [], []
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        try:
            features_scaled = self.scaler.transform(features)
        except:
            # å¦‚æœæ ‡å‡†åŒ–å™¨æœªæ­£ç¡®åŠ è½½ï¼Œä½¿ç”¨ç®€å•æ ‡å‡†åŒ–
            features_scaled = (features - np.mean(features, axis=0)) / (np.std(features, axis=0) + 1e-8)
        
        # é¢„æµ‹
        raw_predictions = []
        raw_confidences = []
        
        if self.model is not None:
            with torch.no_grad():
                features_tensor = torch.FloatTensor(features_scaled).to(self.device)
                outputs = self.model(features_tensor)
                probabilities = torch.softmax(outputs, dim=1).cpu().numpy()
                predicted_labels = np.argmax(probabilities, axis=1)
                raw_confidences = np.max(probabilities, axis=1)
                raw_predictions = predicted_labels + 1  # è½¬æ¢å›1-4
        else:
            # å¦‚æœæ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨ç®€å•è§„åˆ™é¢„æµ‹
            print("è­¦å‘Š: ä½¿ç”¨ç®€å•è§„åˆ™é¢„æµ‹")
            for i, feature_vec in enumerate(features):
                # åŸºäºä»·æ ¼è¶‹åŠ¿çš„ç®€å•è§„åˆ™
                momentum = feature_vec[12]  # momentum_5
                volatility = feature_vec[9]   # volatility_5
                
                if momentum > 0.01 and volatility < 0.02:
                    pred = 1  # åšå¤šå¼€ä»“
                elif momentum < -0.01 and volatility < 0.02:
                    pred = 3  # åšç©ºå¼€ä»“
                elif momentum > 0 and volatility > 0.03:
                    pred = 4  # åšç©ºå¹³ä»“
                else:
                    pred = 2  # åšå¤šå¹³ä»“
                
                raw_predictions.append(pred)
                raw_confidences.append(0.6)
        
        # æ„å»ºåŸå§‹ä¿¡å·åˆ—è¡¨
        raw_signals = []
        for i, (pred, conf, idx) in enumerate(zip(raw_predictions, raw_confidences, indices)):
            raw_signals.append({
                'index': i,
                'label': pred,
                'confidence': conf,
                'data_index': idx
            })
        
        # åº”ç”¨äº¤æ˜“é€»è¾‘è¿‡æ»¤ï¼ˆé«˜ç½®ä¿¡åº¦ + æ¯æ—¥äº¤æ˜“é™åˆ¶ï¼‰
        filtered_signals = self.filter_trading_signals(raw_signals, confidence_threshold=0.7, max_daily_trades=3)
        
        # æå–è¿‡æ»¤åçš„ç»“æœ
        predictions = [signal['label'] for signal in filtered_signals]
        confidences = [signal['confidence'] for signal in filtered_signals]
        filtered_indices = [signal['data_index'] for signal in filtered_signals]
        
        print(f"åŸå§‹ä¿¡å·æ•°: {len(raw_predictions)}, è¿‡æ»¤åä¿¡å·æ•°: {len(predictions)}")
        
        return predictions, confidences, filtered_indices
    
    def create_visualization(self, df, predictions, confidences, indices, filename):
        """
        åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        """
        plt.figure(figsize=(15, 8))
        
        # ç»˜åˆ¶ä»·æ ¼æ›²çº¿
        x_values = df['x'].values
        prices = df['index_value'].values
        
        plt.plot(x_values, prices, 'b-', linewidth=1.5, label='è‚¡æŒ‡ä»·æ ¼', alpha=0.8)
        
        # æ ‡è®°äº¤æ˜“ä¿¡å·
        signal_counts = {1: 0, 2: 0, 3: 0, 4: 0}
        
        for pred, conf, idx in zip(predictions, confidences, indices):
            if idx < len(df):
                x_pos = df['x'].iloc[idx]
                y_pos = df['index_value'].iloc[idx]
                
                # åªæ˜¾ç¤ºé«˜ç½®ä¿¡åº¦çš„ä¿¡å·
                if conf > 0.5:
                    plt.scatter(x_pos, y_pos, 
                              c=self.signal_colors[pred], 
                              marker=self.signal_markers[pred], 
                              s=100, 
                              alpha=0.8,
                              edgecolors='black',
                              linewidth=0.5,
                              label=self.signal_names[pred] if signal_counts[pred] == 0 else "")
                    
                    # æ·»åŠ ç½®ä¿¡åº¦æ ‡æ³¨ï¼ˆå¯é€‰ï¼‰
                    if conf > 0.7:
                        plt.annotate(f'{conf:.2f}', 
                                   (x_pos, y_pos), 
                                   xytext=(5, 5), 
                                   textcoords='offset points',
                                   fontsize=8, 
                                   alpha=0.7)
                    
                    signal_counts[pred] += 1
        
        # å›¾è¡¨è®¾ç½®
        plt.title(f'è‚¡æŒ‡æœŸè´§äº¤æ˜“ä¿¡å·é¢„æµ‹ - {os.path.splitext(filename)[0]}', fontsize=16, fontweight='bold')
        plt.xlabel('æ—¶é—´åºåˆ—', fontsize=12)
        plt.ylabel('è‚¡æŒ‡ä»·æ ¼', fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.legend(loc='upper left', fontsize=10)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        total_signals = sum(signal_counts.values())
        info_text = f'æ€»ä¿¡å·æ•°: {total_signals}\n'
        for signal_type, count in signal_counts.items():
            if count > 0:
                info_text += f'{self.signal_names[signal_type]}: {count}\n'
        
        plt.text(0.02, 0.98, info_text, 
                transform=plt.gca().transAxes, 
                verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),
                fontsize=9)
        
        # ä¿å­˜å›¾ç‰‡
        output_path = f"./result/{os.path.splitext(filename)[0]}.png"
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… å·²ç”Ÿæˆ: {output_path} (ä¿¡å·æ•°: {total_signals})")
        return output_path, signal_counts
    
    def process_all_files(self):
        """
        å¤„ç†æ‰€æœ‰CSVæ–‡ä»¶
        """
        # ç¡®ä¿resultç›®å½•å­˜åœ¨
        os.makedirs('./result', exist_ok=True)
        
        # è·å–labelç›®å½•ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶
        label_dir = './label'
        if not os.path.exists(label_dir):
            print(f"âŒ æ ‡ç­¾ç›®å½•ä¸å­˜åœ¨: {label_dir}")
            return
            
        csv_files = glob.glob('./label/*.csv')
        
        if not csv_files:
            print(f"âŒ åœ¨{label_dir}ç›®å½•ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")
            return
        
        print(f"ğŸ“ åœ¨{label_dir}ç›®å½•ä¸­æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶: {[os.path.basename(f) for f in csv_files]}")
        
        # åŠ è½½æ¨¡å‹
        if not self.load_model():
            print("âš ï¸  æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨ç®€å•è§„åˆ™é¢„æµ‹")
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        total_files = 0
        successful_files = 0
        all_signal_stats = {1: 0, 2: 0, 3: 0, 4: 0}
        
        for csv_file in sorted(csv_files):
            try:
                filename = os.path.basename(csv_file)
                print(f"\nğŸ“Š å¤„ç†æ–‡ä»¶: {filename}")
                
                # è¯»å–æ•°æ®
                df = pd.read_csv(csv_file)
                
                # æ£€æŸ¥å¿…è¦çš„åˆ—
                if 'x' not in df.columns or 'index_value' not in df.columns:
                    print(f"âŒ æ–‡ä»¶ {filename} ç¼ºå°‘å¿…è¦çš„åˆ— (x, index_value)")
                    continue
                
                # é¢„æµ‹ä¿¡å·
                predictions, confidences, indices = self.predict_signals(df)
                
                if len(predictions) == 0:
                    print(f"âš ï¸  æ–‡ä»¶ {filename} æ— æ³•ç”Ÿæˆé¢„æµ‹ä¿¡å·")
                    continue
                
                # åˆ›å»ºå¯è§†åŒ–
                output_path, signal_counts = self.create_visualization(
                    df, predictions, confidences, indices, filename
                )
                
                # ç»Ÿè®¡
                for signal_type, count in signal_counts.items():
                    all_signal_stats[signal_type] += count
                
                successful_files += 1
                
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
                continue
            
            total_files += 1
        
        # è¾“å‡ºæ€»ç»“
        print(f"\n" + "=" * 60)
        print(f"ğŸ¯ å¤„ç†å®Œæˆï¼")
        print(f"   æ€»æ–‡ä»¶æ•°: {total_files}")
        print(f"   æˆåŠŸå¤„ç†: {successful_files}")
        print(f"   ç”Ÿæˆå›¾ç‰‡: {successful_files} å¼ ")
        
        print(f"\nğŸ“ˆ ä¿¡å·ç»Ÿè®¡:")
        total_signals = sum(all_signal_stats.values())
        for signal_type, count in all_signal_stats.items():
            percentage = (count / total_signals * 100) if total_signals > 0 else 0
            print(f"   {self.signal_names[signal_type]}: {count} æ¬¡ ({percentage:.1f}%)")
        
        print(f"\nğŸ“ æ‰€æœ‰å›¾ç‰‡å·²ä¿å­˜åˆ° ./result/ ç›®å½•")

def main():
    print("=== è‚¡æŒ‡æœŸè´§äº¤æ˜“ä¿¡å·é¢„æµ‹ä¸å¯è§†åŒ–ç³»ç»Ÿ ===")
    print("ğŸš€ å¼€å§‹å¤„ç†...")
    
    predictor = TradingSignalPredictor()
    predictor.process_all_files()
    
    print("\nâœ¨ å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    main()